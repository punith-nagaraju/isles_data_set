{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b81ff4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install dependencies\n",
    "%pip install torchio --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a40d85b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 2: Imports\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torchio as tio\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea0ff8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 3: ISLES Dataset Loader (3D volumes for UNet)\n",
    "class ISLESDataset3D(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.samples = []\n",
    "        mask_root = os.path.join(root_dir, \"derivatives\")\n",
    "        for subject in os.listdir(root_dir):\n",
    "            if subject.startswith(\"sub-\"):\n",
    "                ses_dir = os.path.join(root_dir, subject, \"ses-0001\")\n",
    "                if os.path.exists(ses_dir):\n",
    "                    dwi_dir = os.path.join(ses_dir, \"dwi\")\n",
    "                    anat_dir = os.path.join(ses_dir, \"anat\")\n",
    "                    dwi_path = [f for f in os.listdir(dwi_dir) if f.endswith(\"_dwi.nii.gz\")]\n",
    "                    flair_path = [f for f in os.listdir(anat_dir) if f.endswith(\"_FLAIR.nii.gz\")]\n",
    "                    mask_dir = os.path.join(mask_root, subject, \"ses-0001\")\n",
    "                    mask_path = []\n",
    "                    if os.path.exists(mask_dir):\n",
    "                        mask_path = [f for f in os.listdir(mask_dir) if f.endswith(\".nii.gz\")]\n",
    "                    if dwi_path and flair_path and mask_path:\n",
    "                        self.samples.append({\n",
    "                            \"dwi\": os.path.join(dwi_dir, dwi_path[0]),\n",
    "                            \"flair\": os.path.join(anat_dir, flair_path[0]),\n",
    "                            \"mask\": os.path.join(mask_dir, mask_path[0])\n",
    "                        })\n",
    "        print(f\"Total 3D samples: {len(self.samples)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        dwi = nib.load(sample[\"dwi\"]).get_fdata()\n",
    "        flair = nib.load(sample[\"flair\"]).get_fdata()\n",
    "        mask = nib.load(sample[\"mask\"]).get_fdata()\n",
    "        # Crop to minimum shape\n",
    "       # After normalization, before stacking:\n",
    "        crop_shape = (64, 64, 16)  # or smaller if needed\n",
    "        dwi = dwi[:crop_shape[0], :crop_shape[1], :crop_shape[2]]\n",
    "        flair = flair[:crop_shape[0], :crop_shape[1], :crop_shape[2]]\n",
    "        mask = mask[:crop_shape[0], :crop_shape[1], :crop_shape[2]]\n",
    "        # Pad to be divisible by 16\n",
    "        def pad_to_16(arr):\n",
    "            pad = [(0, (16 - s % 16) % 16) for s in arr.shape]\n",
    "            return np.pad(arr, pad, mode='constant')\n",
    "        dwi = pad_to_16(dwi)\n",
    "        flair = pad_to_16(flair)\n",
    "        mask = pad_to_16(mask)\n",
    "        # Normalize\n",
    "        dwi = (dwi - dwi.mean()) / (dwi.std() + 1e-5)\n",
    "        flair = (flair - flair.mean()) / (flair.std() + 1e-5)\n",
    "        x = np.stack([dwi, flair], axis=0).astype(np.float32)\n",
    "        y = (mask > 0).astype(np.float32)\n",
    "        return torch.tensor(x), torch.tensor(y).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76b8c307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 3D samples: 248\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 4: DataLoader\n",
    "dataset = ISLESDataset3D('data')\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be12b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels=2, out_channels=1, init_features=16):\n",
    "        super().__init__()\n",
    "        features = init_features\n",
    "        self.encoder1 = UNet3D._block(in_channels, features)\n",
    "        self.pool1 = nn.MaxPool3d(2)\n",
    "        self.encoder2 = UNet3D._block(features, features * 2)\n",
    "        self.pool2 = nn.MaxPool3d(2)\n",
    "        self.encoder3 = UNet3D._block(features * 2, features * 4)\n",
    "        self.pool3 = nn.MaxPool3d(2)\n",
    "\n",
    "        self.bottleneck = UNet3D._block(features * 4, features * 8)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose3d(features * 8, features * 4, 2, stride=2)\n",
    "        self.decoder3 = UNet3D._block(features * 8, features * 4)\n",
    "        self.up2 = nn.ConvTranspose3d(features * 4, features * 2, 2, stride=2)\n",
    "        self.decoder2 = UNet3D._block(features * 4, features * 2)\n",
    "        self.up1 = nn.ConvTranspose3d(features * 2, features, 2, stride=2)\n",
    "        self.decoder1 = UNet3D._block(features * 2, features)\n",
    "\n",
    "        self.conv = nn.Conv3d(features, out_channels, kernel_size=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _crop_to_match(src, tgt):\n",
    "        src_shape = src.shape[2:]\n",
    "        tgt_shape = tgt.shape[2:]\n",
    "        crop = [(s - t) // 2 for s, t in zip(src_shape, tgt_shape)]\n",
    "        slices = tuple(slice(c, c + t) for c, t in zip(crop, tgt_shape))\n",
    "        return src[(...,) + slices]\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv3d(in_channels, features, 3, padding=1),\n",
    "            nn.BatchNorm3d(features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(features, features, 3, padding=1),\n",
    "            nn.BatchNorm3d(features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool3(enc3))\n",
    "\n",
    "        dec3 = self.up3(bottleneck)\n",
    "        enc3_cropped = self._crop_to_match(enc3, dec3)\n",
    "        dec3 = torch.cat((dec3, enc3_cropped), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        dec2 = self.up2(dec3)\n",
    "        enc2_cropped = self._crop_to_match(enc2, dec2)\n",
    "        dec2 = torch.cat((dec2, enc2_cropped), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        dec1 = self.up1(dec2)\n",
    "        enc1_cropped = self._crop_to_match(enc1, dec1)\n",
    "        dec1 = torch.cat((dec1, enc1_cropped), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        return self.conv(dec1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3530795e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0017ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Cell 6: Model, Loss, Optimizer\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet3D(in_channels=2, out_channels=1, init_features=8).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e594decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 7: Training Loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # ...existing code...\n",
    "        out = model(x)\n",
    "        # Crop y to match out shape\n",
    "        if out.shape != y.shape:\n",
    "            # Compute cropping for each spatial dimension\n",
    "            diff = [y.shape[i] - out.shape[i] for i in range(2, 5)]\n",
    "            crop = [ (d // 2, d - d // 2) for d in diff ]\n",
    "            y_cropped = y[\n",
    "                :,\n",
    "                :,\n",
    "                crop[0][0]:y.shape[2]-crop[0][1] if crop[0][1] > 0 else y.shape[2],\n",
    "                crop[1][0]:y.shape[3]-crop[1][1] if crop[1][1] > 0 else y.shape[3],\n",
    "                crop[2][0]:y.shape[4]-crop[2][1] if crop[2][1] > 0 else y.shape[4],\n",
    "            ]\n",
    "        else:\n",
    "            y_cropped = y\n",
    "        loss = criterion(out, y_cropped)\n",
    "# ...existing code...\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63becb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 8: Prediction and Visualization (middle slice)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x, y = next(iter(loader))\n",
    "    x = x.to(device)\n",
    "    # ...existing code...\n",
    "    pred = torch.sigmoid(model(x)).cpu().numpy()[0,0]\n",
    "    flair = x.cpu().numpy()[0,1]\n",
    "    mask = y.cpu().numpy()[0,0]\n",
    "    # Crop mask to match pred shape\n",
    "    if mask.shape != pred.shape:\n",
    "        diff = [mask.shape[i] - pred.shape[i] for i in range(3)]\n",
    "        crop = [ (d // 2, d - d // 2) for d in diff ]\n",
    "        mask = mask[\n",
    "            crop[0][0]:mask.shape[0]-crop[0][1] if crop[0][1] > 0 else mask.shape[0],\n",
    "            crop[1][0]:mask.shape[1]-crop[1][1] if crop[1][1] > 0 else mask.shape[1],\n",
    "            crop[2][0]:mask.shape[2]-crop[2][1] if crop[2][1] > 0 else mask.shape[2],\n",
    "        ]\n",
    "        flair = flair[\n",
    "            crop[0][0]:flair.shape[0]-crop[0][1] if crop[0][1] > 0 else flair.shape[0],\n",
    "            crop[1][0]:flair.shape[1]-crop[1][1] if crop[1][1] > 0 else flair.shape[1],\n",
    "            crop[2][0]:flair.shape[2]-crop[2][1] if crop[2][1] > 0 else flair.shape[2],\n",
    "        ]\n",
    "# ...existing code...\n",
    "    mid = flair.shape[2] // 2\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title('FLAIR')\n",
    "    plt.imshow(flair[:,:,mid], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.title('Mask')\n",
    "    plt.imshow(mask[:,:,mid], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title('Predicted')\n",
    "    plt.imshow(pred[:,:,mid] > 0.5, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
