{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Import libraries **"
      ],
      "metadata": {
        "id": "XLtYlbRFan6K"
      },
      "id": "XLtYlbRFan6K"
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "# %%\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "!pip install albumentations\n",
        "!pip install nibabel\n",
        "!pip install opencv-python\n",
        "!pip install segmentation_models_pytorch # Includes UNet, DeepLabV3+, etc.\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install tensorflow # Keep if you need it for other parts, but not for these models\n",
        "\n",
        "import nibabel as nib\n",
        "from torch.utils.data import Dataset\n",
        "import tensorflow as tf # Keep if needed\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import warnings\n",
        "\n",
        "# Import models from segmentation_models_pytorch\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "print(f\"Imports done\")\n"
      ],
      "metadata": {
        "outputId": "55f96468-de20-4914-da0b-987863820851",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gCSw-ihESEQ"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.8)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.15.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.11.5)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.11.0.86)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (3.12.5)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (6.4.9)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (5.3.2)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel) (6.5.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from nibabel) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from nibabel) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel) (4.14.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: segmentation_models_pytorch in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (11.2.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (0.5.3)\n",
            "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (1.0.15)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (1.1.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation_models_pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->segmentation_models_pytorch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2025.4.26)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Imports done\n"
          ]
        }
      ],
      "id": "9gCSw-ihESEQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2 : All helper functions and DataLoader"
      ],
      "metadata": {
        "id": "_PwmKrSAakgU"
      },
      "id": "_PwmKrSAakgU"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# 1. Set up paths and parameters\n",
        "DATA_DIR = \"/content/drive/My Drive/data/ISLES-2022\"  # or your actual data folder path\n",
        "N_MODELS = 1 # Train 3 models for the ensemble\n",
        "EPOCHS = 1 # Reduce epochs for testing, increase for proper training\n",
        "BATCH_SIZE = 2 # Reduce batch size due to model complexity and 3D data\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_paths_dict = {}\n",
        "\n",
        "\n",
        "print(\"Using :\", DEVICE)\n",
        "# ...existing code...\n",
        "\n",
        "TARGET_SHAPE = (64, 64, 64)  # Choose a shape your GPU can handle - might need adjustment for 3D\n",
        "\n",
        "\n",
        "# Helper functions (Your existing code)\n",
        "def dice_loss(pred, target, smooth=1.):\n",
        "    pred = pred.reshape(-1)\n",
        "    target = target.reshape(-1)\n",
        "    intersection = (pred * target).sum()\n",
        "    return 1 - ((2. * intersection + smooth) / (pred.sum() + target.sum() + smooth))\n",
        "\n",
        "def dice_score(pred, target, smooth=1.):\n",
        "    pred = pred.reshape(-1)\n",
        "    target = target.reshape(-1)\n",
        "    intersection = (pred * target).sum()\n",
        "    return (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
        "\n",
        "def plot_sample(x, y, pred, channel=0):\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title(\"Input (selected channel)\")\n",
        "    plt.imshow(x[channel].cpu(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(\"Ground Truth Mask\")\n",
        "    plt.imshow(y.squeeze().cpu(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(\"Prediction\")\n",
        "    plt.imshow(pred.squeeze().cpu(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def safe_unsqueeze_mask(y):\n",
        "    # Ensure mask is [B, 1, D, H, W]\n",
        "    if y.ndim == 4:\n",
        "        y = y.unsqueeze(1)\n",
        "    elif y.ndim == 5 and y.shape[1] != 1:\n",
        "        # If mask has extra channels, take the first\n",
        "        y = y[:, :1, ...]\n",
        "    elif y.ndim < 4:\n",
        "        raise ValueError(f\"Mask shape too small: {y.shape}\")\n",
        "    return y\n",
        "\n",
        "# ...existing code...\n",
        "def pad_or_crop_to_shape_2d(x, target_shape_2d):\n",
        "    # x: [B, C, H, W]\n",
        "    _, _, H, W = x.shape\n",
        "    tH, tW = target_shape_2d\n",
        "    # Pad\n",
        "    pad_h_before = (tH - H) // 2\n",
        "    pad_h_after = tH - H - pad_h_before\n",
        "    pad_w_before = (tW - W) // 2\n",
        "    pad_w_after = tW - W - pad_w_before\n",
        "\n",
        "    # Padding order for F.pad is (W_left, W_right, H_top, H_bottom) for 4D\n",
        "    x = F.pad(x, [pad_w_before, pad_w_after, pad_h_before, pad_h_after])\n",
        "\n",
        "    # Crop (only if needed, padding should handle this)\n",
        "    x = x[:, :, :tH, :tW]\n",
        "    return x\n",
        "# Make sure to include the pad_or_crop_to_shape helper function:\n",
        "def pad_or_crop_to_shape(x, target_shape):\n",
        "    # x: [B, C, D, H, W]\n",
        "    _, _, D, H, W = x.shape\n",
        "    tD, tH, tW = target_shape\n",
        "    # Pad\n",
        "    pad_d_before = (tD - D) // 2\n",
        "    pad_d_after = tD - D - pad_d_before\n",
        "    pad_h_before = (tH - H) // 2\n",
        "    pad_h_after = tH - H - pad_h_before\n",
        "    pad_w_before = (tW - W) // 2\n",
        "    pad_w_after = tW - W - pad_w_before\n",
        "\n",
        "    # Padding order is (W_left, W_right, H_top, H_bottom, D_front, D_back) for 5D\n",
        "    x = F.pad(x, [pad_w_before, pad_w_after, pad_h_before, pad_h_after, pad_d_before, pad_d_after])\n",
        "\n",
        "    # Crop (only if needed, padding should handle this)\n",
        "    x = x[:, :, :tD, :tH, :tW]\n",
        "    return x\n",
        "\n",
        "\n",
        "# Evaluation Metrics (Add AVD and F1 score functions here - if you have them)\n",
        "def absolute_volume_difference(pred, target):\n",
        "    # Implement AVD calculation\n",
        "    # Example (assuming pred and target are binary tensors):\n",
        "    pred_volume = torch.sum(pred)\n",
        "    target_volume = torch.sum(target)\n",
        "    if target_volume == 0: # Avoid division by zero\n",
        "        return float('inf') if pred_volume > 0 else 0.0\n",
        "    avd = torch.abs(pred_volume - target_volume) / target_volume * 100\n",
        "    return avd.item()\n",
        "\n",
        "\n",
        "def lesion_wise_f1_score(pred, target):\n",
        "    # Implement lesion-wise F1 score calculation\n",
        "    # This requires identifying individual lesions. A common approach is\n",
        "    # to use connected components analysis on the ground truth and predictions.\n",
        "    # This is more complex and requires libraries like SciPy or OpenCv.\n",
        "    # For a simple pixel-wise F1, you can calculate that.\n",
        "\n",
        "    # Simple pixel-wise F1:\n",
        "    pred = pred.reshape(-1).bool()\n",
        "    target = target.reshape(-1).bool()\n",
        "\n",
        "    tp = (pred & target).sum().item()\n",
        "    fp = (pred & ~target).sum().item()\n",
        "    fn = (~pred & target).sum().item()\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-8)\n",
        "    recall = tp / (tp + fn + 1e-8)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
        "\n",
        "    return f1\n",
        "\n",
        "\n",
        "def pad_or_crop_5d(x, target_shape):\n",
        "    # x: [B, C, D, H, W] or [B, 1, D, H, W]\n",
        "    _, _, D, H, W = x.shape\n",
        "    tD, tH, tW = target_shape\n",
        "    # Pad\n",
        "    pad_d = max(tD - D, 0)\n",
        "    pad_h = max(tH - H, 0)\n",
        "    pad_w = max(tW - W, 0)\n",
        "    x = F.pad(x, [0, pad_w, 0, pad_h, 0, pad_d])\n",
        "    # Crop\n",
        "    x = x[:, :, :tD, :tH, :tW]\n",
        "    return x\n",
        "\n",
        "\n",
        "# ...existing code...\n",
        "\n",
        "def safe_pad_or_crop(x, target_shape):\n",
        "    # Accepts [B, C, D, H, W] or [B, D, H, W] or [C, D, H, W]\n",
        "    if x.ndim == 4:\n",
        "        x = x.unsqueeze(0)  # Add batch dim if missing\n",
        "    if x.ndim == 5:\n",
        "        _, _, D, H, W = x.shape\n",
        "        tD, tH, tW = target_shape\n",
        "        pad_d = max(tD - D, 0)\n",
        "        pad_h = max(tH - H, 0)\n",
        "        pad_w = max(tW - W, 0)\n",
        "        x = F.pad(x, [0, pad_w, 0, pad_h, 0, pad_d])\n",
        "        x = x[:, :, :tD, :tH, :tW]\n",
        "    else:\n",
        "        raise ValueError(f\"Input shape not supported: {x.shape}\")\n",
        "    return x\n",
        "\n",
        "# %%\n",
        "class ISLESDataset3D(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.samples = []\n",
        "        print(f\"entering 3D samples\")\n",
        "        mask_root = os.path.join(root_dir, \"derivatives\")\n",
        "        for subject in os.listdir(root_dir):\n",
        "            if subject.startswith(\"sub-\"):\n",
        "                ses_dir = os.path.join(root_dir, subject, \"ses-0001\")\n",
        "                if os.path.exists(ses_dir):\n",
        "                    dwi_dir = os.path.join(ses_dir, \"dwi\")\n",
        "                    anat_dir = os.path.join(ses_dir, \"anat\")\n",
        "\n",
        "                    # Look for both dwi and adc files in the dwi directory\n",
        "                    dwi_files = [f for f in os.listdir(dwi_dir) if f.endswith(\"_dwi.nii.gz\")]\n",
        "                    adc_files = [f for f in os.listdir(dwi_dir) if f.endswith(\"_adc.nii.gz\")]\n",
        "\n",
        "                    flair_path = [f for f in os.listdir(anat_dir) if f.endswith(\"_FLAIR.nii.gz\")]\n",
        "\n",
        "                    mask_dir = os.path.join(mask_root, subject, \"ses-0001\")\n",
        "                    mask_path = []\n",
        "                    if os.path.exists(mask_dir):\n",
        "                        mask_path = [f for f in os.listdir(mask_dir) if f.endswith(\".nii.gz\")]\n",
        "\n",
        "                    # Ensure all required files are found\n",
        "                    if dwi_files and adc_files and flair_path and mask_path:\n",
        "                         # For this ensemble, we'll use DWI and ADC as inputs (2 channels)\n",
        "                         # If you want to include FLAIR, you'll need to modify the dataset\n",
        "                         # and the model input channels.\n",
        "                        self.samples.append({\n",
        "                            \"dwi\": os.path.join(dwi_dir, dwi_files[0]),\n",
        "                            \"adc\": os.path.join(dwi_dir, adc_files[0]), # Add ADC path\n",
        "                            \"mask\": os.path.join(mask_dir, mask_path[0])\n",
        "                        })\n",
        "        print(f\"Total 3D samples: {len(self.samples)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        dwi = self.load_nifti(sample[\"dwi\"])    # [H, W, D]\n",
        "        adc = self.load_nifti(sample[\"adc\"])    # [H, W, D] - Load ADC\n",
        "\n",
        "        # Crop all to the minimum shape\n",
        "        min_shape = np.minimum.reduce([dwi.shape, adc.shape])\n",
        "        dwi_cropped = dwi[:min_shape[0], :min_shape[1], :min_shape[2]]\n",
        "        adc_cropped = adc[:min_shape[0], :min_shape[1], :min_shape[2]]\n",
        "\n",
        "\n",
        "        x = np.stack([dwi_cropped, adc_cropped], axis=0)  # Stack DWI and ADC [2, H, W, D]\n",
        "        y = self.load_nifti(sample[\"mask\"])\n",
        "        y = y[:min_shape[0], :min_shape[1], :min_shape[2]]  # Crop mask to match\n",
        "\n",
        "\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def load_nifti(path):\n",
        "        return np.asarray(nib.load(path).get_fdata(), dtype=np.float32)\n",
        "\n",
        "# %%\n",
        "\n",
        "\n",
        "def load_ensemble(model_paths_dict, device):\n",
        "    models = []\n",
        "    for model_name, paths in model_paths_dict.items():\n",
        "         # Get config to recreate the model structure\n",
        "        arch_name = model_name.split('_')[0] # Extract architecture name\n",
        "        encoder_name = '_'.join(model_name.split('_')[1:]) # Extract encoder name\n",
        "\n",
        "        for path in paths:\n",
        "            try:\n",
        "                 # Recreate the model structure\n",
        "                 model = get_smp_model(\n",
        "                    arch=arch_name,\n",
        "                    encoder_name=encoder_name,\n",
        "                    in_channels=2, # DWI and ADC\n",
        "                    out_classes=1  # Binary segmentation\n",
        "                 ).to(device)\n",
        "\n",
        "                 # Load the state dictionary\n",
        "                 model.load_state_dict(torch.load(path, map_location=device))\n",
        "                 model.eval() # Set model to evaluation mode\n",
        "                 models.append(model)\n",
        "                 print(f\"Loaded {path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading model from {path}: {e}\")\n",
        "                # Decide how to handle loading errors (e.g., skip the model)\n",
        "                continue\n",
        "    return models\n",
        "\n",
        "\n",
        "# Helper functions for visualization (adapt if needed for 3D slices)\n",
        "def plot_sample_colored(x, y, pred, channel=0, slice_idx=None):\n",
        "    # Adapting for 3D: need to select a slice\n",
        "    if x.ndim == 5: # [B, C, D, H, W]\n",
        "        if slice_idx is None:\n",
        "             slice_idx = x.shape[2] // 2 # Use middle slice if not specified\n",
        "        img_slice = x[0, channel, slice_idx].cpu().squeeze() # Assuming batch size 1 for visualization\n",
        "        y_slice = y[0, 0, slice_idx].cpu().squeeze() # Assuming batch size 1, 1 channel for mask\n",
        "        pred_slice = pred[0, 0, slice_idx].cpu().squeeze() # Assuming batch size 1, 1 channel for prediction\n",
        "    elif x.ndim == 4: # Assume [C, D, H, W] for single sample visualization\n",
        "         if slice_idx is None:\n",
        "             slice_idx = x.shape[1] // 2\n",
        "         img_slice = x[channel, slice_idx].cpu().squeeze()\n",
        "         y_slice = y[0, slice_idx].cpu().squeeze() # Assuming 1 channel for mask\n",
        "         pred_slice = pred[0, slice_idx].cpu().squeeze() # Assuming 1 channel for prediction\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported input shape for plotting: {x.shape}\")\n",
        "\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "    img = img_slice\n",
        "    img = (img - img.min()) / (img.max() - img.min() + 1e-8) # Normalize\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    ax.imshow(y_slice, cmap='Greens', alpha=0.3)      # Ground truth in green\n",
        "    ax.imshow(pred_slice, cmap='Reds', alpha=0.3)     # Prediction in red\n",
        "    ax.set_title(\"Input + GT (green) + Pred (red)\")\n",
        "    ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_sample_separate_modalities(x, y, pred, slice_idx=None):\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        if x.ndim == 5: # [B, C, D, H, W]\n",
        "            if slice_idx is None:\n",
        "                 slice_idx = x.shape[2] // 2\n",
        "            dwi_img_slice = x[0, 0, slice_idx].cpu().squeeze() # Assuming batch size 1, DWI channel 0\n",
        "            adc_img_slice = x[0, 1, slice_idx].cpu().squeeze() # Assuming batch size 1, ADC channel 1\n",
        "            y_slice = y[0, 0, slice_idx].cpu().squeeze()\n",
        "            pred_slice = pred[0, 0, slice_idx].cpu().squeeze()\n",
        "        elif x.ndim == 4: # Assume [C, D, H, W] for single sample visualization\n",
        "             if slice_idx is None:\n",
        "                 slice_idx = x.shape[1] // 2\n",
        "             dwi_img_slice = x[0, slice_idx].cpu().squeeze()\n",
        "             adc_img_slice = x[1, slice_idx].cpu().squeeze()\n",
        "             y_slice = y[0, slice_idx].cpu().squeeze()\n",
        "             pred_slice = pred[0, slice_idx].cpu().squeeze()\n",
        "        else:\n",
        "             raise ValueError(f\"Unsupported input shape for plotting: {x.shape}\")\n",
        "\n",
        "\n",
        "        # Normalize for better visualization\n",
        "        dwi_img_slice = (dwi_img_slice - dwi_img_slice.min()) / (dwi_img_slice.max() - dwi_img_slice.min() + 1e-8)\n",
        "        adc_img_slice = (adc_img_slice - adc_img_slice.min()) / (adc_img_slice.max() - adc_img_slice.min() + 1e-8)\n",
        "\n",
        "\n",
        "        # Plot DWI Input\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow(dwi_img_slice, cmap='gray')\n",
        "        plt.title(f\"Input (DWI) - Slice {slice_idx}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        # Plot ADC Input\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow(adc_img_slice, cmap='gray')\n",
        "        plt.title(f\"Input (ADC) - Slice {slice_idx}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        # Plot Ground Truth\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow(dwi_img_slice, cmap='gray') # Optionally show DWI in background\n",
        "        plt.imshow(y_slice, cmap='Greens', alpha=0.3)\n",
        "        plt.title(f\"Ground Truth Mask (green) - Slice {slice_idx}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        # Plot Prediction\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow(dwi_img_slice, cmap='gray') # Optionally show DWI in background\n",
        "        plt.imshow(pred_slice, cmap='Reds', alpha=0.3)\n",
        "        plt.title(f\"Prediction Mask (red) - Slice {slice_idx}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def ensemble_predict(models, x, target_shape):\n",
        "        with torch.no_grad():\n",
        "            aligned_preds = []\n",
        "            for i, model in enumerate(models):\n",
        "                # Ensure model is on the correct device\n",
        "                model.to(x.device)\n",
        "                out = model(x) # Model predicts probabilities\n",
        "\n",
        "                 # Ensure out is 5D [B, C, D, H, W] for pad_or_crop_to_shape\n",
        "                while out.ndim < 5:\n",
        "                    out = out.unsqueeze(0) # Add batch or channel dim if missing\n",
        "                # Assuming smp models output [B, C, H, W] or similar, you'll need to adapt for 3D [B, C, D, H, W]\n",
        "                # If smp models are strictly 2D, this approach needs significant modification\n",
        "                # For 3D data, the output should also be 3D [B, C, D, H, W]\n",
        "\n",
        "                # Pad/crop output to match target shape\n",
        "                out_aligned = pad_or_crop_to_shape(out, target_shape)\n",
        "\n",
        "                aligned_preds.append(out_aligned)\n",
        "\n",
        "                # --- Add this print statement ---\n",
        "                print(f\"Model {i} output min/max:\", out_aligned.min().item(), out_aligned.max().item())\n",
        "                # --- End of print statement ---\n",
        "\n",
        "        # Stack the aligned predictions from each model\n",
        "        # All tensors in aligned_preds must have the same shape for stacking\n",
        "        stacked = torch.stack(aligned_preds, dim=0) # Stacks along a new dimension (number of models)\n",
        "\n",
        "        # Average the predictions across the models\n",
        "        avg = torch.mean(stacked, dim=0)\n",
        "\n",
        "        print(\"Averaged probabilities min/max:\", avg.min().item(), avg.max().item())\n",
        "\n",
        "\n",
        "        # Threshold the averaged probabilities to get the final binary mask\n",
        "        final = (avg > 0.5).float() # Use a threshold, e.g., 0.5\n",
        "\n",
        "        return final, avg # Return both the binary mask and averaged probabilities\n",
        "\n",
        "# ...existing code...\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def pad_collate(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    # print(\"Shapes of individual tensors in ys before padding:\", [y.shape for y in ys]) # Keep for debugging if needed\n",
        "\n",
        "    # Find max height, width, depth in this batch\n",
        "    # individual tensors in xs are [C, D, H, W], ys are [D, H, W]\n",
        "    max_d = max(x.shape[1] for x in xs) # Depth\n",
        "    max_h = max(x.shape[2] for x in xs) # Height\n",
        "    max_w = max(x.shape[3] for x in xs) # Width\n",
        "\n",
        "    xs_padded = []\n",
        "    ys_padded = []\n",
        "    for x, y in zip(xs, ys):\n",
        "        # x is [C, D, H, W], y is [D, H, W]\n",
        "\n",
        "        # Padding needs to be calculated based on the current tensor's spatial dimensions\n",
        "        pad_d = max_d - x.shape[1] # Pad in Depth\n",
        "        pad_h = max_h - x.shape[2] # Pad in Height\n",
        "        pad_w = max_w - x.shape[3] # Pad in Width\n",
        "\n",
        "        # Padding order for F.pad is (W_left, W_right, H_top, H_bottom, D_front, D_back) for 5D\n",
        "        # Applied to [C, D, H, W], it pads W, H, then D. For 4D tensor x: (W_left, W_right, H_top, H_bottom, D_front, D_back)\n",
        "        x_padded = F.pad(x, (0, pad_w, 0, pad_h, 0, pad_d))\n",
        "        # print(\"Shape of x_padded AFTER F.pad:\", x_padded.shape) # Debug print\n",
        "\n",
        "        # For y [D, H, W], padding order is (W_left, W_right, H_top, H_bottom, D_front, D_back)\n",
        "        y_padded = F.pad(y, (0, pad_w, 0, pad_h, 0, pad_d)) # This pads the 3D mask\n",
        "        # print(\"Shape of y_padded AFTER F.pad:\", y_padded.shape) # Debug print\n",
        "\n",
        "\n",
        "        y_padded = y_padded.unsqueeze(0) # Add channel dim at index 0 [1, D, H, W]\n",
        "        # print(\"Shape of y_padded AFTER unsqueeze:\", y_padded.shape) # Debug print\n",
        "\n",
        "\n",
        "        xs_padded.append(x_padded)\n",
        "        ys_padded.append(y_padded)\n",
        "\n",
        "    # --- Stack xs_padded here ---\n",
        "    xs_padded = torch.stack(xs_padded, dim=0) # Stacks [C, D_p, H_p, W_p] to [B, C, D_p, H_p, W_p]\n",
        "    # print(\"Shape of xs_padded AFTER stack:\", xs_padded.shape) # Debug print\n",
        "    # --- End of stacking xs_padded ---\n",
        "\n",
        "    ys_padded = torch.stack(ys_padded, dim=0) # Stacks [1, D_p, H_p, W_p] to [B, 1, D_p, H_p, W_p]\n",
        "    # print(\"Shape of ys_padded AFTER stack:\", ys_padded.shape) # Debug print\n",
        "\n",
        "    return xs_padded, ys_padded # Both are now stacked tensors\n",
        "\n",
        "# Remove your custom UNet3D, VNet, NNUnet3D, VNetBlock, NNUnet3DBlock definitions here\n",
        "# We will use models from segmentation_models_pytorch\n"
      ],
      "metadata": {
        "id": "1yptYyujZmWQ",
        "outputId": "eb855dde-ed74-46dc-ca48-cffaa4966489",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1yptYyujZmWQ",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using : cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 3: DATALOADING AND TRAINING"
      ],
      "metadata": {
        "id": "YX0Z9_wIcJY1"
      },
      "id": "YX0Z9_wIcJY1"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# %% Data Loading\n",
        "train_dataset = ISLESDataset3D(\n",
        "    root_dir=DATA_DIR)\n",
        "sample_x, sample_y = train_dataset[0]\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0, # Set to 0 for Colab, higher on systems with multiprocessing\n",
        "    collate_fn=pad_collate\n",
        ")\n",
        "print(f\"Number of samples in dataset: {len(train_dataset)}\")\n",
        "print(f\"Number of batches in DataLoader: {len(train_loader)}\")\n",
        "\n",
        "# %% Model Definitions using segmentation_models_pytorch\n",
        "\n",
        "# Helper function to get models from smp\n",
        "def get_smp_model(arch, encoder_name, in_channels, out_classes):\n",
        "    if arch == \"unet\":\n",
        "        model = smp.Unet(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=\"imagenet\", # You can use pre-trained weights if available for the encoder\n",
        "            in_channels=in_channels,\n",
        "            classes=out_classes,\n",
        "            encoder_depth=5, # Adjust encoder depth if needed\n",
        "            decoder_channels=[256, 128, 64, 32, 16] # Adjust decoder channels if needed\n",
        "        )\n",
        "    elif arch == \"unetplusplus\":\n",
        "         model = smp.UnetPlusPlus(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=\"imagenet\",\n",
        "            in_channels=in_channels,\n",
        "            classes=out_classes,\n",
        "            encoder_depth=5,\n",
        "            decoder_channels=[256, 128, 64, 32, 16]\n",
        "        )\n",
        "    elif arch == \"deeplabv3plus\":\n",
        "        model = smp.DeepLabV3Plus(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=\"imagenet\",\n",
        "            in_channels=in_channels,\n",
        "            classes=out_classes,\n",
        "            encoder_output_stride=16 # Common setting for DeeplabV3+\n",
        "        )\n",
        "    elif arch == \"linknet\":\n",
        "         model = smp.Linknet(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=\"imagenet\",\n",
        "            in_channels=in_channels,\n",
        "            classes=out_classes\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported architecture from smp: {arch}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define the model architectures and their configurations for the ensemble\n",
        "# Using popular backbones from torchvision suitable for segmentation tasks\n",
        "model_architectures = {\n",
        "    \"unet_resnet18\": {\"arch\": \"unet\", \"encoder_name\": \"resnet18\"},\n",
        "    \"deeplabv3plus_resnet50\": {\"arch\": \"deeplabv3plus\", \"encoder_name\": \"resnet50\"},\n",
        "    \"unetplusplus_resnet34\": {\"arch\": \"unetplusplus\", \"encoder_name\": \"resnet34\"},\n",
        "    # You can add more or different models here\n",
        "    # \"unet_resnet34\": {\"arch\": \"unet\", \"encoder_name\": \"resnet34\"},\n",
        "     \"linknet_resnet18\": {\"arch\": \"linknet\", \"encoder_name\": \"resnet18\"}\n",
        "}\n",
        "\n",
        "# %% Training\n",
        "print(f\"#Train multiple base models for ensemble\")\n",
        "\n",
        "\n",
        "# ... previous imports and setup ...\n",
        "\n",
        "for model_name, config in model_architectures.items():\n",
        "    arch_name = config[\"arch\"]\n",
        "    encoder_name = config[\"encoder_name\"]\n",
        "\n",
        "    model_paths_dict[model_name] = []\n",
        "    for i in range(N_MODELS):\n",
        "        print(f\"Training {model_name} model {i+1}/{N_MODELS}, epoch 1/{EPOCHS}\")\n",
        "\n",
        "        # Get the 2D model from smp\n",
        "        try:\n",
        "             # Note: input channels is still 2 (DWI + ADC), output classes is 1\n",
        "             # The model expects 2D input [B, C, H, W]\n",
        "             model = get_smp_model(\n",
        "                 arch=arch_name,\n",
        "                 encoder_name=encoder_name,\n",
        "                 in_channels=2,\n",
        "                 out_classes=1\n",
        "             ).to(DEVICE)\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating {model_name}: {e}\")\n",
        "            continue # Skip to the next model if creation fails\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "        criterion = dice_loss # Still using Dice Loss\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            for batch_idx, (x_3d, y_3d) in enumerate(train_loader):\n",
        "                # x_3d is [B, C, D, H, W]\n",
        "                # y_3d is [B, 1, D, H, W]\n",
        "\n",
        "                # Process slice by slice\n",
        "                batch_losses = []\n",
        "                for d in range(x_3d.shape[2]): # Iterate through depth dimension\n",
        "                    x_slice = x_3d[:, :, d, :, :].to(DEVICE, dtype=torch.float) # Get 2D slice [B, C, H, W]\n",
        "                    y_slice = y_3d[:, :, d, :, :].to(DEVICE, dtype=torch.float) # Get 2D mask slice [B, 1, H, W]\n",
        "\n",
        "                    # --- Add padding to make height and width divisible by the required divisor ---\n",
        "                    # The required divisor depends on the specific model architecture.\n",
        "                    # For UnetPlusPlus with ResNet34, it seems to be 32.\n",
        "                    # For DeepLabV3+ with ResNet50, it was 16.\n",
        "                    # A robust approach would be to check the model's requirement or use a sufficiently large divisor (e.g., 32).\n",
        "\n",
        "                    required_divisor = 32 # Use 32 for this model\n",
        "                    h, w = x_slice.shape[2:]\n",
        "                    new_h = (h + required_divisor - 1) // required_divisor * required_divisor\n",
        "                    new_w = (w + required_divisor - 1) // required_divisor * required_divisor\n",
        "                    target_padded_shape_slice = (new_h, new_w)\n",
        "\n",
        "                    # Use pad_or_crop_to_shape_2d to pad the input slice\n",
        "                    x_slice_padded = pad_or_crop_to_shape_2d(x_slice, target_padded_shape_slice)\n",
        "                    # Also pad the target mask slice to match the padded input slice's spatial shape\n",
        "                    y_slice_padded = pad_or_crop_to_shape_2d(y_slice, target_padded_shape_slice)\n",
        "                    # --- End of padding ---\n",
        "\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    out_slice = model(x_slice_padded) # Use the padded slice as input\n",
        "\n",
        "                    # Now, 'out_slice' has spatial dimensions related to 'target_padded_shape_slice'\n",
        "                    # We need to align 'out_slice' with the original (or padded by pad_collate)\n",
        "                    # spatial shape of 'y_slice' for loss calculation.\n",
        "                    # The 'y_slice' already has spatial dimensions from the original padding in pad_collate.\n",
        "                    # So, align 'out_slice' to 'y_slice' spatial shape.\n",
        "\n",
        "                    target_spatial_shape_slice_for_loss = y_slice.shape[2:] # Original slice shape (H, W)\n",
        "                    # Ensure out_slice is 4D [B, C, H, W] before padding/cropping\n",
        "                    while out_slice.ndim < 4:\n",
        "                         out_slice = out_slice.unsqueeze(0)\n",
        "\n",
        "                    out_slice_aligned_for_loss = pad_or_crop_to_shape_2d(out_slice, target_spatial_shape_slice_for_loss)\n",
        "\n",
        "                    # Ensure target slice y_slice is [B, 1, H, W] and matches the aligned output's spatial shape\n",
        "                    # y_slice already has the correct spatial shape due to pad_collate\n",
        "\n",
        "                    # Ensure both tensors have the same spatial dimensions for loss calculation\n",
        "                    min_spatial_shape_slice = [min(a, b) for a, b in zip(out_slice_aligned_for_loss.shape[2:], y_slice_padded.shape[2:])] # Use y_slice_padded here\n",
        "                    out_slice_aligned_for_loss = out_slice_aligned_for_loss[:, :, :min_spatial_shape_slice[0], :min_spatial_shape_slice[1]]\n",
        "                    y_slice_padded = y_slice_padded[:, :, :min_spatial_shape_slice[0], :min_spatial_shape_slice[1]]\n",
        "\n",
        "\n",
        "                    loss = criterion(out_slice_aligned_for_loss, y_slice_padded) # Calculate loss using the aligned output and padded target\n",
        "                    loss.backward()\n",
        "                    batch_losses.append(loss.item())\n",
        "\n",
        "                # ... rest of the batch loop ...\n",
        "\n",
        "                # ... rest of the batch loop ...\n",
        "\n",
        "                # After processing all slices in the batch, step the optimizer\n",
        "                optimizer.step()\n",
        "                total_loss += np.mean(batch_losses) # Average loss across slices for the batch\n",
        "\n",
        "                print(f\"  {model_name} Model {i+1} Epoch {epoch+1} Batch {batch_idx}/{len(train_loader)} Avg Slice Loss: {np.mean(batch_losses):.4f}\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1} Avg Slice Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "        # Save the trained model\n",
        "    save_path = f\"/content/drive/My Drive/{model_name}_new_model_{i}.pth\"\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    model_paths_dict[model_name].append(save_path)\n",
        "    print(f\"Saved {save_path}\")\n"
      ],
      "metadata": {
        "id": "gyUbowiFbxfE",
        "outputId": "16a32890-e67c-41d8-a0cc-5ffef6a59547",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "39a290b1ebc248c386415f568b941476",
            "2374ec3b5dd240baa86aebb441039120",
            "7c7f493c7bf246d0a55fb2afd14de36b",
            "4cd212ab4ef348f19225032415a399f8",
            "8d571893a63843e5ae190b30b95bce7f",
            "7d879717e64f438e807d67c4db795aeb",
            "51abac2b5993406ab1ae1cc8380e197c",
            "f2c49cc64f394beebbb604e728e3b3fd",
            "660214638995483986b0c915e6c4315f",
            "fb3c85fa51ed418189aa4834a8581810",
            "c40b9be0d0f84419b790e61b5de52e96",
            "7f3afdfc254f49e1807bbeab903d4315",
            "34ded42d82cc4eb985dea3067576dec7",
            "c8d73cb633d94bd3bd0e423c3695b648",
            "e0b8fedb8a434cfeb1f228b5a3bdb88f",
            "d9c021f95e794e26a94e535b81c47a14",
            "f6da4decc39040979b0dc053342a4a6c",
            "5c3c4e81a9594d989a75c2f671a0b075",
            "a79712743dfa4ccdbdc17e8d0c862bec",
            "cca10ec3af39425d873a8bb534146211",
            "3b5f84b5b86946bfac06068f076a5d97",
            "7d4bf38aff304b1390342ec33307cb5f"
          ]
        }
      },
      "id": "gyUbowiFbxfE",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "entering 3D samples\n",
            "Total 3D samples: 250\n",
            "Number of samples in dataset: 250\n",
            "Number of batches in DataLoader: 125\n",
            "#Train multiple base models for ensemble\n",
            "Training unet_resnet18 model 1/1, epoch 1/1\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 0/125 Avg Slice Loss: 0.9986\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 1/125 Avg Slice Loss: 0.9995\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 2/125 Avg Slice Loss: 0.9903\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 3/125 Avg Slice Loss: 0.9822\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 4/125 Avg Slice Loss: 0.9984\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 5/125 Avg Slice Loss: 0.9992\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 6/125 Avg Slice Loss: 0.9957\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 7/125 Avg Slice Loss: 0.9931\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 8/125 Avg Slice Loss: 0.9970\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 9/125 Avg Slice Loss: 0.9953\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 10/125 Avg Slice Loss: 0.9721\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 11/125 Avg Slice Loss: 0.9978\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 12/125 Avg Slice Loss: 0.9648\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 13/125 Avg Slice Loss: 0.9715\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 14/125 Avg Slice Loss: 1.0001\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 15/125 Avg Slice Loss: 0.9989\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 16/125 Avg Slice Loss: 0.9933\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 17/125 Avg Slice Loss: 0.9637\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 18/125 Avg Slice Loss: 0.9997\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 19/125 Avg Slice Loss: 0.9928\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 20/125 Avg Slice Loss: 0.9965\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 21/125 Avg Slice Loss: 0.9997\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 22/125 Avg Slice Loss: 0.9991\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 23/125 Avg Slice Loss: 0.9986\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 24/125 Avg Slice Loss: 0.9977\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 25/125 Avg Slice Loss: 0.9745\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 26/125 Avg Slice Loss: 0.9902\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 27/125 Avg Slice Loss: 0.9877\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 28/125 Avg Slice Loss: 0.9393\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 29/125 Avg Slice Loss: 0.9999\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 30/125 Avg Slice Loss: 0.9946\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 31/125 Avg Slice Loss: 0.9944\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 32/125 Avg Slice Loss: 0.9980\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 33/125 Avg Slice Loss: 0.9993\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 34/125 Avg Slice Loss: 0.9960\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 35/125 Avg Slice Loss: 0.9853\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 36/125 Avg Slice Loss: 0.9987\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 37/125 Avg Slice Loss: 0.9945\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 38/125 Avg Slice Loss: 0.9702\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 39/125 Avg Slice Loss: 0.9943\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 40/125 Avg Slice Loss: 0.9910\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 41/125 Avg Slice Loss: 0.9966\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 42/125 Avg Slice Loss: 0.9991\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 43/125 Avg Slice Loss: 0.9934\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 44/125 Avg Slice Loss: 0.9964\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 45/125 Avg Slice Loss: 0.9919\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 46/125 Avg Slice Loss: 0.9951\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 47/125 Avg Slice Loss: 0.9957\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 48/125 Avg Slice Loss: 0.9838\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 49/125 Avg Slice Loss: 0.9987\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 50/125 Avg Slice Loss: 0.9999\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 51/125 Avg Slice Loss: 0.9878\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 52/125 Avg Slice Loss: 0.9999\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 53/125 Avg Slice Loss: 0.9847\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 54/125 Avg Slice Loss: 0.9996\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 55/125 Avg Slice Loss: 0.9776\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 56/125 Avg Slice Loss: 0.9985\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 57/125 Avg Slice Loss: 0.9875\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 58/125 Avg Slice Loss: 0.9995\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 59/125 Avg Slice Loss: 0.9896\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 60/125 Avg Slice Loss: 0.9996\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 61/125 Avg Slice Loss: 0.9997\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 62/125 Avg Slice Loss: 0.9838\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 63/125 Avg Slice Loss: 0.9964\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 64/125 Avg Slice Loss: 0.9816\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 65/125 Avg Slice Loss: 0.9751\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 66/125 Avg Slice Loss: 0.9983\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 67/125 Avg Slice Loss: 0.9982\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 68/125 Avg Slice Loss: 0.9991\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 69/125 Avg Slice Loss: 0.9991\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 70/125 Avg Slice Loss: 0.9819\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 71/125 Avg Slice Loss: 0.9976\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 72/125 Avg Slice Loss: 0.9741\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 73/125 Avg Slice Loss: 0.9862\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 74/125 Avg Slice Loss: 0.9748\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 75/125 Avg Slice Loss: 0.9999\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 76/125 Avg Slice Loss: 0.9952\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 77/125 Avg Slice Loss: 0.9991\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 78/125 Avg Slice Loss: 0.9968\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 79/125 Avg Slice Loss: 0.9967\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 80/125 Avg Slice Loss: 0.9891\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 81/125 Avg Slice Loss: 0.9992\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 82/125 Avg Slice Loss: 0.9991\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 83/125 Avg Slice Loss: 0.9942\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 84/125 Avg Slice Loss: 0.9997\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 85/125 Avg Slice Loss: 0.9996\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 86/125 Avg Slice Loss: 0.9995\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 87/125 Avg Slice Loss: 0.9796\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 88/125 Avg Slice Loss: 0.9973\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 89/125 Avg Slice Loss: 0.9989\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 90/125 Avg Slice Loss: 0.9992\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 91/125 Avg Slice Loss: 0.9981\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 92/125 Avg Slice Loss: 0.9989\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 93/125 Avg Slice Loss: 0.9993\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 94/125 Avg Slice Loss: 0.9960\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 95/125 Avg Slice Loss: 0.9984\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 96/125 Avg Slice Loss: 0.9971\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 97/125 Avg Slice Loss: 0.9959\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 98/125 Avg Slice Loss: 0.9990\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 99/125 Avg Slice Loss: 0.9746\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 100/125 Avg Slice Loss: 0.9922\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 101/125 Avg Slice Loss: 0.9998\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 102/125 Avg Slice Loss: 0.9837\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 103/125 Avg Slice Loss: 0.9961\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 104/125 Avg Slice Loss: 0.9922\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 105/125 Avg Slice Loss: 0.9988\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 106/125 Avg Slice Loss: 0.9927\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 107/125 Avg Slice Loss: 0.9993\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 108/125 Avg Slice Loss: 0.9992\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 109/125 Avg Slice Loss: 0.9955\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 110/125 Avg Slice Loss: 0.9994\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 111/125 Avg Slice Loss: 0.9140\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 112/125 Avg Slice Loss: 0.9979\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 113/125 Avg Slice Loss: 0.9991\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 114/125 Avg Slice Loss: 0.9997\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 115/125 Avg Slice Loss: 0.9992\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 116/125 Avg Slice Loss: 0.9989\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 117/125 Avg Slice Loss: 0.9990\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 118/125 Avg Slice Loss: 0.9999\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 119/125 Avg Slice Loss: 0.9996\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 120/125 Avg Slice Loss: 0.9986\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 121/125 Avg Slice Loss: 0.9962\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 122/125 Avg Slice Loss: 0.9948\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 123/125 Avg Slice Loss: 0.9973\n",
            "  unet_resnet18 Model 1 Epoch 1 Batch 124/125 Avg Slice Loss: 0.9995\n",
            "Epoch 1 Avg Slice Loss: 0.9925\n",
            "Saved /content/drive/My Drive/unet_resnet18_new_model_0.pth\n",
            "Training deeplabv3plus_resnet50 model 1/1, epoch 1/1\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 0/125 Avg Slice Loss: 0.9993\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 1/125 Avg Slice Loss: 0.9999\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 2/125 Avg Slice Loss: 0.9989\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 3/125 Avg Slice Loss: 0.9999\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 4/125 Avg Slice Loss: 0.9786\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 5/125 Avg Slice Loss: 0.9976\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 6/125 Avg Slice Loss: 0.9968\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 7/125 Avg Slice Loss: 0.9872\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 8/125 Avg Slice Loss: 0.9994\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 9/125 Avg Slice Loss: 0.9853\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 10/125 Avg Slice Loss: 0.9992\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 11/125 Avg Slice Loss: 0.9999\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 12/125 Avg Slice Loss: 0.9788\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 13/125 Avg Slice Loss: 0.9914\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 14/125 Avg Slice Loss: 0.9998\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 15/125 Avg Slice Loss: 0.9991\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 16/125 Avg Slice Loss: 0.9979\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 17/125 Avg Slice Loss: 0.9986\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 18/125 Avg Slice Loss: 0.9998\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 19/125 Avg Slice Loss: 0.9849\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 20/125 Avg Slice Loss: 0.9935\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 21/125 Avg Slice Loss: 0.9977\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 22/125 Avg Slice Loss: 0.9784\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 23/125 Avg Slice Loss: 0.9979\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 24/125 Avg Slice Loss: 0.9987\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 25/125 Avg Slice Loss: 0.9998\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 26/125 Avg Slice Loss: 0.9920\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 27/125 Avg Slice Loss: 0.9983\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 28/125 Avg Slice Loss: 0.9994\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 29/125 Avg Slice Loss: 0.9945\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 30/125 Avg Slice Loss: 0.9997\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 31/125 Avg Slice Loss: 0.9984\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 32/125 Avg Slice Loss: 0.9991\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 33/125 Avg Slice Loss: 0.9985\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 34/125 Avg Slice Loss: 0.9991\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 35/125 Avg Slice Loss: 0.9998\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 36/125 Avg Slice Loss: 0.9887\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 37/125 Avg Slice Loss: 0.9984\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 38/125 Avg Slice Loss: 0.9957\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 39/125 Avg Slice Loss: 0.9915\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 40/125 Avg Slice Loss: 0.9965\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 41/125 Avg Slice Loss: 1.0000\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 42/125 Avg Slice Loss: 0.9968\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 43/125 Avg Slice Loss: 0.9815\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 44/125 Avg Slice Loss: 0.9935\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 45/125 Avg Slice Loss: 0.9973\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 46/125 Avg Slice Loss: 0.9979\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 47/125 Avg Slice Loss: 0.9614\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 48/125 Avg Slice Loss: 0.9911\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 49/125 Avg Slice Loss: 0.9975\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 50/125 Avg Slice Loss: 0.9984\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 51/125 Avg Slice Loss: 0.9967\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 52/125 Avg Slice Loss: 0.9989\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 53/125 Avg Slice Loss: 0.9713\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 54/125 Avg Slice Loss: 0.9976\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 55/125 Avg Slice Loss: 0.9997\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 56/125 Avg Slice Loss: 0.9984\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 57/125 Avg Slice Loss: 0.9987\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 58/125 Avg Slice Loss: 0.9875\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 59/125 Avg Slice Loss: 0.9720\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 60/125 Avg Slice Loss: 0.9988\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 61/125 Avg Slice Loss: 0.9997\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 62/125 Avg Slice Loss: 0.9750\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 63/125 Avg Slice Loss: 0.9917\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 64/125 Avg Slice Loss: 0.9897\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 65/125 Avg Slice Loss: 0.9995\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 66/125 Avg Slice Loss: 0.9967\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 67/125 Avg Slice Loss: 0.9991\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 68/125 Avg Slice Loss: 0.9768\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 69/125 Avg Slice Loss: 0.9869\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 70/125 Avg Slice Loss: 0.9996\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 71/125 Avg Slice Loss: 0.9988\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 72/125 Avg Slice Loss: 0.9915\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 73/125 Avg Slice Loss: 0.9847\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 74/125 Avg Slice Loss: 0.9972\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 75/125 Avg Slice Loss: 0.9981\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 76/125 Avg Slice Loss: 0.9993\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 77/125 Avg Slice Loss: 0.9973\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 78/125 Avg Slice Loss: 0.9985\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 79/125 Avg Slice Loss: 0.9990\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 80/125 Avg Slice Loss: 0.9955\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 81/125 Avg Slice Loss: 0.9998\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 82/125 Avg Slice Loss: 0.9994\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 83/125 Avg Slice Loss: 0.8963\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 84/125 Avg Slice Loss: 0.9992\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 85/125 Avg Slice Loss: 0.9947\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 86/125 Avg Slice Loss: 0.9989\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 87/125 Avg Slice Loss: 0.9956\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 88/125 Avg Slice Loss: 0.9995\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 89/125 Avg Slice Loss: 0.9994\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 90/125 Avg Slice Loss: 0.9881\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 91/125 Avg Slice Loss: 0.9538\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 92/125 Avg Slice Loss: 0.9995\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 93/125 Avg Slice Loss: 0.9856\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 94/125 Avg Slice Loss: 0.9978\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 95/125 Avg Slice Loss: 0.9991\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 96/125 Avg Slice Loss: 0.9801\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 97/125 Avg Slice Loss: 0.9982\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 98/125 Avg Slice Loss: 0.9998\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 99/125 Avg Slice Loss: 0.9955\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 100/125 Avg Slice Loss: 0.9999\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 101/125 Avg Slice Loss: 0.9953\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 102/125 Avg Slice Loss: 0.9926\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 103/125 Avg Slice Loss: 0.9743\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 104/125 Avg Slice Loss: 0.9955\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 105/125 Avg Slice Loss: 0.9993\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 106/125 Avg Slice Loss: 0.9989\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 107/125 Avg Slice Loss: 0.9992\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 108/125 Avg Slice Loss: 0.9912\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 109/125 Avg Slice Loss: 0.9988\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 110/125 Avg Slice Loss: 0.9985\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 111/125 Avg Slice Loss: 0.9807\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 112/125 Avg Slice Loss: 0.9986\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 113/125 Avg Slice Loss: 0.9714\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 114/125 Avg Slice Loss: 0.9890\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 115/125 Avg Slice Loss: 0.9952\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 116/125 Avg Slice Loss: 0.9955\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 117/125 Avg Slice Loss: 0.9999\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 118/125 Avg Slice Loss: 0.9970\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 119/125 Avg Slice Loss: 0.9990\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 120/125 Avg Slice Loss: 0.9919\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 121/125 Avg Slice Loss: 0.9989\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 122/125 Avg Slice Loss: 0.9989\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 123/125 Avg Slice Loss: 0.9961\n",
            "  deeplabv3plus_resnet50 Model 1 Epoch 1 Batch 124/125 Avg Slice Loss: 0.9991\n",
            "Epoch 1 Avg Slice Loss: 0.9932\n",
            "Saved /content/drive/My Drive/deeplabv3plus_resnet50_new_model_0.pth\n",
            "Training unetplusplus_resnet34 model 1/1, epoch 1/1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39a290b1ebc248c386415f568b941476"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/87.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f3afdfc254f49e1807bbeab903d4315"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 0/125 Avg Slice Loss: 0.9593\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 1/125 Avg Slice Loss: 0.9923\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 2/125 Avg Slice Loss: 0.9998\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 3/125 Avg Slice Loss: 0.9769\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 4/125 Avg Slice Loss: 0.9704\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 5/125 Avg Slice Loss: 1.0005\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 6/125 Avg Slice Loss: 0.9972\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 7/125 Avg Slice Loss: 1.0042\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 8/125 Avg Slice Loss: 1.0004\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 9/125 Avg Slice Loss: 1.0006\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 10/125 Avg Slice Loss: 1.0006\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 11/125 Avg Slice Loss: 0.9929\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 12/125 Avg Slice Loss: 1.0001\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 13/125 Avg Slice Loss: 1.0004\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 14/125 Avg Slice Loss: 1.0163\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 15/125 Avg Slice Loss: 0.9998\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 16/125 Avg Slice Loss: 0.9999\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 17/125 Avg Slice Loss: 1.0008\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 18/125 Avg Slice Loss: 1.0001\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 19/125 Avg Slice Loss: 1.0104\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 20/125 Avg Slice Loss: 1.0088\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 21/125 Avg Slice Loss: 1.0016\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 22/125 Avg Slice Loss: 1.0034\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 23/125 Avg Slice Loss: 1.0015\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 24/125 Avg Slice Loss: 1.0056\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 25/125 Avg Slice Loss: 0.9983\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 26/125 Avg Slice Loss: 1.0010\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 27/125 Avg Slice Loss: 0.9995\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 28/125 Avg Slice Loss: 0.9992\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 29/125 Avg Slice Loss: 1.0065\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 30/125 Avg Slice Loss: 1.0005\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 31/125 Avg Slice Loss: 1.0000\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 32/125 Avg Slice Loss: 1.0016\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 33/125 Avg Slice Loss: 1.0022\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 34/125 Avg Slice Loss: 0.9993\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 35/125 Avg Slice Loss: 1.0000\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 36/125 Avg Slice Loss: 1.0004\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 37/125 Avg Slice Loss: 0.9998\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 38/125 Avg Slice Loss: 1.0028\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 39/125 Avg Slice Loss: 1.0012\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 40/125 Avg Slice Loss: 0.9995\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 41/125 Avg Slice Loss: 1.0004\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 42/125 Avg Slice Loss: 0.9999\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 43/125 Avg Slice Loss: 1.0132\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 44/125 Avg Slice Loss: 1.0005\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 45/125 Avg Slice Loss: 0.9998\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 46/125 Avg Slice Loss: 1.0018\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 47/125 Avg Slice Loss: 1.0066\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 48/125 Avg Slice Loss: 1.0099\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 49/125 Avg Slice Loss: 1.0007\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 50/125 Avg Slice Loss: 1.0361\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 51/125 Avg Slice Loss: 1.0001\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 52/125 Avg Slice Loss: 1.0011\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 53/125 Avg Slice Loss: 1.0002\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 54/125 Avg Slice Loss: 1.0056\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 55/125 Avg Slice Loss: 1.0005\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 56/125 Avg Slice Loss: 1.0055\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 57/125 Avg Slice Loss: 1.0014\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 58/125 Avg Slice Loss: 1.0008\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 59/125 Avg Slice Loss: 1.0000\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 60/125 Avg Slice Loss: 0.9999\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 61/125 Avg Slice Loss: 1.0023\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 62/125 Avg Slice Loss: 0.9997\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 63/125 Avg Slice Loss: 1.0008\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 64/125 Avg Slice Loss: 1.0023\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 65/125 Avg Slice Loss: 1.0032\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 66/125 Avg Slice Loss: 1.0018\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 67/125 Avg Slice Loss: 1.0001\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 68/125 Avg Slice Loss: 1.0001\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 69/125 Avg Slice Loss: 1.0003\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 70/125 Avg Slice Loss: 1.0048\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 71/125 Avg Slice Loss: 1.0007\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 72/125 Avg Slice Loss: 1.0163\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 73/125 Avg Slice Loss: 1.0002\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 74/125 Avg Slice Loss: 1.0006\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 75/125 Avg Slice Loss: 1.0009\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 76/125 Avg Slice Loss: 1.0010\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 77/125 Avg Slice Loss: 1.0043\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 78/125 Avg Slice Loss: 1.0061\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 79/125 Avg Slice Loss: 1.0001\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 80/125 Avg Slice Loss: 1.0027\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 81/125 Avg Slice Loss: 1.0000\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 82/125 Avg Slice Loss: 1.0006\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 83/125 Avg Slice Loss: 1.0010\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 84/125 Avg Slice Loss: 1.0006\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 85/125 Avg Slice Loss: 1.0026\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 86/125 Avg Slice Loss: 1.0164\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 87/125 Avg Slice Loss: 0.9998\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 88/125 Avg Slice Loss: 0.9978\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 89/125 Avg Slice Loss: 1.0000\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 90/125 Avg Slice Loss: 1.0003\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 91/125 Avg Slice Loss: 1.0002\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 92/125 Avg Slice Loss: 1.0019\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 93/125 Avg Slice Loss: 1.0008\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 94/125 Avg Slice Loss: 1.0007\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 95/125 Avg Slice Loss: 1.0081\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 96/125 Avg Slice Loss: 1.0005\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 97/125 Avg Slice Loss: 1.0021\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 98/125 Avg Slice Loss: 1.0024\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 99/125 Avg Slice Loss: 0.9996\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 100/125 Avg Slice Loss: 1.0003\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 101/125 Avg Slice Loss: 1.0016\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 102/125 Avg Slice Loss: 0.9993\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 103/125 Avg Slice Loss: 1.0003\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 104/125 Avg Slice Loss: 1.0016\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 105/125 Avg Slice Loss: 1.0004\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 106/125 Avg Slice Loss: 1.0000\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 107/125 Avg Slice Loss: 1.0000\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 108/125 Avg Slice Loss: 1.0004\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 109/125 Avg Slice Loss: 1.0002\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 110/125 Avg Slice Loss: 1.0014\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 111/125 Avg Slice Loss: 1.0001\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 112/125 Avg Slice Loss: 1.0262\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 113/125 Avg Slice Loss: 0.9997\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 114/125 Avg Slice Loss: 0.9997\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 115/125 Avg Slice Loss: 1.0001\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 116/125 Avg Slice Loss: 1.0011\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 117/125 Avg Slice Loss: 1.0048\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 118/125 Avg Slice Loss: 1.0003\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 119/125 Avg Slice Loss: 1.0005\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 120/125 Avg Slice Loss: 1.0002\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 121/125 Avg Slice Loss: 0.9984\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 122/125 Avg Slice Loss: 1.0007\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 123/125 Avg Slice Loss: 1.0015\n",
            "  unetplusplus_resnet34 Model 1 Epoch 1 Batch 124/125 Avg Slice Loss: 0.9987\n",
            "Epoch 1 Avg Slice Loss: 1.0013\n",
            "Saved /content/drive/My Drive/unetplusplus_resnet34_new_model_0.pth\n",
            "Training linknet_resnet18 model 1/1, epoch 1/1\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 0/125 Avg Slice Loss: 0.9975\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 1/125 Avg Slice Loss: 0.9932\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 2/125 Avg Slice Loss: 0.9479\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 3/125 Avg Slice Loss: 1.0000\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 4/125 Avg Slice Loss: 0.9971\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 5/125 Avg Slice Loss: 0.9553\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 6/125 Avg Slice Loss: 0.9864\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 7/125 Avg Slice Loss: 0.9994\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 8/125 Avg Slice Loss: 0.9999\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 9/125 Avg Slice Loss: 0.9971\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 10/125 Avg Slice Loss: 0.9664\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 11/125 Avg Slice Loss: 0.9839\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 12/125 Avg Slice Loss: 0.9999\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 13/125 Avg Slice Loss: 0.9972\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 14/125 Avg Slice Loss: 0.9937\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 15/125 Avg Slice Loss: 1.0000\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 16/125 Avg Slice Loss: 0.9981\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 17/125 Avg Slice Loss: 0.9994\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 18/125 Avg Slice Loss: 0.9997\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 19/125 Avg Slice Loss: 0.9967\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 20/125 Avg Slice Loss: 0.9982\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 21/125 Avg Slice Loss: 0.9987\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 22/125 Avg Slice Loss: 0.9907\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 23/125 Avg Slice Loss: 0.9967\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 24/125 Avg Slice Loss: 0.9985\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 25/125 Avg Slice Loss: 0.9978\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 26/125 Avg Slice Loss: 0.9992\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 27/125 Avg Slice Loss: 0.9949\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 28/125 Avg Slice Loss: 0.9988\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 29/125 Avg Slice Loss: 0.9978\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 30/125 Avg Slice Loss: 0.9996\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 31/125 Avg Slice Loss: 0.9881\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 32/125 Avg Slice Loss: 0.9796\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 33/125 Avg Slice Loss: 0.9985\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 34/125 Avg Slice Loss: 0.9965\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 35/125 Avg Slice Loss: 0.9974\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 36/125 Avg Slice Loss: 0.9999\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 37/125 Avg Slice Loss: 0.9994\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 38/125 Avg Slice Loss: 0.9908\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 39/125 Avg Slice Loss: 0.9999\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 40/125 Avg Slice Loss: 0.9986\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 41/125 Avg Slice Loss: 0.9953\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 42/125 Avg Slice Loss: 0.9818\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 43/125 Avg Slice Loss: 0.9989\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 44/125 Avg Slice Loss: 0.9898\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 45/125 Avg Slice Loss: 0.9993\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 46/125 Avg Slice Loss: 0.9826\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 47/125 Avg Slice Loss: 0.9845\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 48/125 Avg Slice Loss: 0.9749\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 49/125 Avg Slice Loss: 0.9968\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 50/125 Avg Slice Loss: 0.9976\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 51/125 Avg Slice Loss: 0.9963\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 52/125 Avg Slice Loss: 0.9957\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 53/125 Avg Slice Loss: 0.9931\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 54/125 Avg Slice Loss: 0.9998\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 55/125 Avg Slice Loss: 0.9692\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 56/125 Avg Slice Loss: 0.9986\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 57/125 Avg Slice Loss: 0.9993\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 58/125 Avg Slice Loss: 0.9993\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 59/125 Avg Slice Loss: 0.9976\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 60/125 Avg Slice Loss: 0.9986\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 61/125 Avg Slice Loss: 0.9990\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 62/125 Avg Slice Loss: 0.9965\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 63/125 Avg Slice Loss: 1.0002\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 64/125 Avg Slice Loss: 0.9992\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 65/125 Avg Slice Loss: 0.9986\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 66/125 Avg Slice Loss: 0.9999\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 67/125 Avg Slice Loss: 0.9982\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 68/125 Avg Slice Loss: 0.9991\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 69/125 Avg Slice Loss: 0.9999\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 70/125 Avg Slice Loss: 0.9786\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 71/125 Avg Slice Loss: 0.9994\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 72/125 Avg Slice Loss: 0.9999\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 73/125 Avg Slice Loss: 0.9959\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 74/125 Avg Slice Loss: 0.9952\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 75/125 Avg Slice Loss: 0.9941\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 76/125 Avg Slice Loss: 0.9948\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 77/125 Avg Slice Loss: 0.9996\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 78/125 Avg Slice Loss: 0.9989\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 79/125 Avg Slice Loss: 0.9965\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 80/125 Avg Slice Loss: 0.9997\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 81/125 Avg Slice Loss: 0.9996\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 82/125 Avg Slice Loss: 0.9976\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 83/125 Avg Slice Loss: 0.9996\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 84/125 Avg Slice Loss: 0.9985\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 85/125 Avg Slice Loss: 0.9380\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 86/125 Avg Slice Loss: 0.9995\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 87/125 Avg Slice Loss: 1.0000\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 88/125 Avg Slice Loss: 0.9921\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 89/125 Avg Slice Loss: 0.9798\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 90/125 Avg Slice Loss: 0.9961\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 91/125 Avg Slice Loss: 0.9863\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 92/125 Avg Slice Loss: 0.9964\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 93/125 Avg Slice Loss: 0.9989\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 94/125 Avg Slice Loss: 0.9962\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 95/125 Avg Slice Loss: 0.9973\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 96/125 Avg Slice Loss: 0.9927\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 97/125 Avg Slice Loss: 0.9998\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 98/125 Avg Slice Loss: 0.9917\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 99/125 Avg Slice Loss: 0.9947\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 100/125 Avg Slice Loss: 0.9990\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 101/125 Avg Slice Loss: 0.9961\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 102/125 Avg Slice Loss: 0.9952\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 103/125 Avg Slice Loss: 1.0000\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 104/125 Avg Slice Loss: 0.9986\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 105/125 Avg Slice Loss: 0.9996\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 106/125 Avg Slice Loss: 0.9968\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 107/125 Avg Slice Loss: 0.9979\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 108/125 Avg Slice Loss: 0.9824\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 109/125 Avg Slice Loss: 0.9725\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 110/125 Avg Slice Loss: 0.9982\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 111/125 Avg Slice Loss: 0.9997\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 112/125 Avg Slice Loss: 0.9999\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 113/125 Avg Slice Loss: 0.9999\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 114/125 Avg Slice Loss: 0.9994\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 115/125 Avg Slice Loss: 0.9991\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 116/125 Avg Slice Loss: 0.9957\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 117/125 Avg Slice Loss: 0.9986\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 118/125 Avg Slice Loss: 0.9923\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 119/125 Avg Slice Loss: 0.9953\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 120/125 Avg Slice Loss: 0.9952\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 121/125 Avg Slice Loss: 0.9998\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 122/125 Avg Slice Loss: 0.9983\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 123/125 Avg Slice Loss: 1.0000\n",
            "  linknet_resnet18 Model 1 Epoch 1 Batch 124/125 Avg Slice Loss: 0.9898\n",
            "Epoch 1 Avg Slice Loss: 0.9942\n",
            "Saved /content/drive/My Drive/linknet_resnet18_new_model_0.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "V7v28hA_z4J8"
      },
      "id": "V7v28hA_z4J8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging (Bootstrap Aggregating) type of ensemble with model averaging.\n",
        "**bold text**\n",
        "Here's the breakdown:\n",
        "\n",
        "Ensemble Learning: The core idea is to combine the predictions of multiple individual models (also called base learners or committee members) to improve overall performance and robustness [1].\n",
        "\n",
        "Bagging: This technique involves training multiple models on different subsets of the training data (created by sampling with replacement, although in your code, it appears you are using the full dataset for each model). The key characteristic of bagging is that the base models are trained independently of each other.\n",
        "\n",
        "Model Averaging: In your ensemble_predict function (which is not fully shown in this snippet but implied by the averaging of predictions), the predictions from the individual trained models are combined by taking their average probability outputs. For segmentation tasks, this is a common way to combine predictions from different models. The final binary mask is then obtained by thresholding the averaged probabilities (e.g., (avg > 0.5).float()).\n",
        "\n",
        "Why it's Bagging with Model Averaging:\n",
        "\n",
        "You are training multiple models (UNet3D, VNet, NNUnet3D).\n",
        "These models are trained independently of each other.\n",
        "Their predictions are combined by averaging the output probabilities."
      ],
      "metadata": {
        "id": "YKJHkdPv-8oL"
      },
      "id": "YKJHkdPv-8oL"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BItLs-hxcXWa"
      },
      "id": "BItLs-hxcXWa"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# **Step 5: Load All Trained Models and Perform Ensemble Prediction**\n",
        "# %%\n",
        "print(f\"# Loading All Trained Models from model_paths_dict for Ensemble Prediction\")\n",
        "\n",
        "# Helper function to get models from smp\n",
        "def get_smp_model(arch, encoder_name, in_channels, out_classes):\n",
        "    if arch == \"unet\":\n",
        "        model = smp.Unet(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=\"imagenet\",\n",
        "            in_channels=in_channels,\n",
        "            classes=out_classes,\n",
        "            encoder_depth=5,\n",
        "            decoder_channels=[256, 128, 64, 32, 16]\n",
        "        )\n",
        "    elif arch == \"unetplusplus\":\n",
        "         model = smp.UnetPlusPlus(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=\"imagenet\",\n",
        "            in_channels=in_channels,\n",
        "            classes=out_classes,\n",
        "            encoder_depth=5,\n",
        "            decoder_channels=[256, 128, 64, 32, 16]\n",
        "        )\n",
        "    elif arch == \"deeplabv3plus\":\n",
        "        model = smp.DeepLabV3Plus(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=\"imagenet\",\n",
        "            in_channels=in_channels,\n",
        "            classes=out_classes,\n",
        "            encoder_output_stride=16\n",
        "        )\n",
        "    elif arch == \"linknet\":\n",
        "         model = smp.Linknet(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=\"imagenet\",\n",
        "            in_channels=in_channels,\n",
        "            classes=out_classes\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported architecture from smp: {arch}\")\n",
        "\n",
        "    return model\n",
        "model_architectures = {\n",
        "    \"unet_resnet18\": {\"arch\": \"unet\", \"encoder_name\": \"resnet18\"},\n",
        "    \"deeplabv3plus_resnet50\": {\"arch\": \"deeplabv3plus\", \"encoder_name\": \"resnet50\"},\n",
        "    \"unetplusplus_resnet34\": {\"arch\": \"unetplusplus\", \"encoder_name\": \"resnet34\"},\n",
        "    # You can add more or different models here\n",
        "    # \"unet_resnet34\": {\"arch\": \"unet\", \"encoder_name\": \"resnet34\"},\n",
        "     \"linknet_resnet18\": {\"arch\": \"linknet\", \"encoder_name\": \"resnet18\"}\n",
        "}\n",
        "# Define the base path to your Google Drive\n",
        "DRIVE_PATH = \"/content/drive/My Drive/\"\n",
        "\n",
        "# Initialize model_paths_dict if it doesn't exist\n",
        "if 'model_paths_dict' not in globals():\n",
        "    model_paths_dict = {}\n",
        "\n",
        "# Assign the specified model paths to model_paths_dict\n",
        "model_paths_dict[\"unet_resnet18\"] = [os.path.join(DRIVE_PATH, \"unet_resnet18_new_model_0.pth\")]\n",
        "model_paths_dict[\"unetplusplus_resnet34\"] = [os.path.join(DRIVE_PATH, \"unetplusplus_resnet34_new_model_0.pth\")]\n",
        "model_paths_dict[\"deeplabv3plus_resnet50\"] = [os.path.join(DRIVE_PATH, \"deeplabv3plus_resnet50_new_model_0.pth\")]\n",
        "model_paths_dict[\"linknet_resnet18\"] = [os.path.join(DRIVE_PATH, \"linknet_resnet18_new_model_0.pth\")]\n",
        "\n",
        "# You can print the dictionary to verify\n",
        "print(\"model_paths_dict after assignment:\")\n",
        "print(model_paths_dict)\n",
        "all_loaded_models = []\n",
        "train_dataset = ISLESDataset3D(\n",
        "    root_dir=DATA_DIR)\n",
        "sample_x, sample_y = train_dataset[0]\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0, # Set to 0 for Colab, higher on systems with multiprocessing\n",
        "    collate_fn=pad_collate\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "import torch\n",
        "\n",
        "def ensemble_predict_slice_by_slice(models, x_3d, target_shape_3d):\n",
        "        # ... (previous code) ...\n",
        "        with torch.no_grad():\n",
        "            batch_predictions = []\n",
        "            for b in range(x_3d.shape[0]):\n",
        "                 volume_predictions = []\n",
        "                 for d in range(x_3d.shape[2]):\n",
        "                     x_slice = x_3d[b:b+1, :, d:d+1, :, :].to(DEVICE, dtype=torch.float)\n",
        "                     x_slice = x_slice.squeeze(2)\n",
        "\n",
        "                     required_divisor = 32\n",
        "                     h, w = x_slice.shape[2:]\n",
        "                     new_h = (h + required_divisor - 1) // required_divisor * required_divisor\n",
        "                     new_w = (w + required_divisor - 1) // required_divisor * required_divisor\n",
        "                     target_padded_shape_slice = (new_h, new_w)\n",
        "\n",
        "                     x_slice_padded = pad_or_crop_to_shape_2d(x_slice, target_padded_shape_slice)\n",
        "\n",
        "\n",
        "                     aligned_preds_slice = []\n",
        "                     for i, model in enumerate(models):\n",
        "                         try:\n",
        "                             model.to(x_slice_padded.device)\n",
        "                             out_slice = model(x_slice_padded)\n",
        "\n",
        "                             # --- Add Sigmoid Activation ---\n",
        "                             out_slice = torch.sigmoid(out_slice)\n",
        "                             # --- End of Add Sigmoid Activation ---\n",
        "\n",
        "                             while out_slice.ndim < 4:\n",
        "                                 out_slice = out_slice.unsqueeze(0)\n",
        "\n",
        "                             target_spatial_shape_slice_for_alignment = target_shape_3d[1:]\n",
        "                             out_slice_aligned = pad_or_crop_to_shape_2d(out_slice, target_spatial_shape_slice_for_alignment)\n",
        "\n",
        "                             aligned_preds_slice.append(out_slice_aligned)\n",
        "                         except Exception as e:\n",
        "                             print(f\"Error predicting with model {i} on slice d={d}, batch b={b}: {e}\")\n",
        "                             continue\n",
        "\n",
        "                     if not aligned_preds_slice:\n",
        "                         print(f\"Error: aligned_preds_slice is empty for batch {b}, slice {d}. Skipping stacking for this slice.\")\n",
        "                         if target_shape_3d:\n",
        "                              h_target, w_target = target_shape_3d[1:]\n",
        "                              dummy_slice = torch.zeros(1, 1, 1, h_target, w_target, device=DEVICE, dtype=torch.float)\n",
        "                              volume_predictions.append(dummy_slice)\n",
        "                         else:\n",
        "                             print(\"Cannot append dummy slice: target_shape_3d is not available.\")\n",
        "\n",
        "                     else:\n",
        "                          stacked_slice = torch.stack(aligned_preds_slice, dim=0)\n",
        "                          avg_slice = torch.mean(stacked_slice, dim=0)\n",
        "                          avg_slice = avg_slice.unsqueeze(2)\n",
        "                          volume_predictions.append(avg_slice)\n",
        "\n",
        "\n",
        "                 if volume_predictions:\n",
        "                     volume_predictions = torch.cat(volume_predictions, dim=2)\n",
        "                     batch_predictions.append(volume_predictions)\n",
        "                 else:\n",
        "                     print(f\"Warning: No volume predictions for batch {b}. Skipping batch.\")\n",
        "\n",
        "\n",
        "            if batch_predictions:\n",
        "                 averaged_probs_batch = torch.cat(batch_predictions, dim=0)\n",
        "\n",
        "                 print(\"Averaged probabilities min/max:\", averaged_probs_batch.min().item(), averaged_probs_batch.max().item())\n",
        "\n",
        "                 # The thresholding for the final mask should now work correctly with probabilities\n",
        "                 final_mask_batch = (averaged_probs_batch > 0.5).float()\n",
        "\n",
        "                 return final_mask_batch, averaged_probs_batch\n",
        "            else:\n",
        "                 print(\"Error: batch_predictions is empty. Returning None or handling appropriately.\")\n",
        "                 return None, None\n",
        "\n",
        "def plot_sample_colored(x, y, pred, channel=0, slice_idx=None):\n",
        "    print(\"Inside plot_sample_colored function\")\n",
        "\n",
        "    x_orig_shape = x.shape\n",
        "    y_orig_shape = y.shape\n",
        "    pred_orig_shape = pred.shape\n",
        "\n",
        "    x = x.cpu().squeeze()\n",
        "    y = y.cpu().squeeze()\n",
        "    pred = pred.cpu().squeeze()\n",
        "\n",
        "    print(f\"Original shapes: x={x_orig_shape}, y={y_orig_shape}, pred={pred_orig_shape}\")\n",
        "    print(f\"Shapes after squeezing: x={x.shape}, y={y.shape}, pred={pred.shape}\")\n",
        "    print(f\"x.ndim after squeezing: {x.ndim}\")\n",
        "    print(f\"pred.ndim after squeezing: {pred.ndim}\") # Added print\n",
        "    print(f\"slice_idx: {slice_idx}\") # Added print\n",
        "\n",
        "\n",
        "    img_slice = None\n",
        "    y_slice = None\n",
        "    pred_slice = None\n",
        "\n",
        "    if x.ndim == 4:\n",
        "        print(\"Entering x.ndim == 4 block\")\n",
        "        if slice_idx is None:\n",
        "             slice_idx = x.shape[1] // 2\n",
        "        img_slice = x[channel, slice_idx]\n",
        "        y_slice = y[slice_idx] if y.ndim > 2 else y\n",
        "\n",
        "        # --- More robust assignment of pred_slice ---\n",
        "        try:\n",
        "            if pred.ndim > 2:\n",
        "                pred_slice = pred[slice_idx]\n",
        "                print(f\"  Assigned pred_slice using pred[{slice_idx}]. Shape: {pred_slice.shape if pred_slice is not None else 'None'}\") # Added print\n",
        "            else:\n",
        "                pred_slice = pred\n",
        "                print(f\"  Assigned pred_slice using pred. Shape: {pred_slice.shape if pred_slice is not None else 'None'}\") # Added print\n",
        "        except Exception as e:\n",
        "            print(f\"Error during pred_slice assignment in ndim 4 block: {e}\") # Added error print\n",
        "            pred_slice = None # Explicitly set to None on error\n",
        "        # --- End of robust assignment ---\n",
        "\n",
        "        print(f\"  slice_idx: {slice_idx}, img_slice shape: {img_slice.shape if img_slice is not None else 'None'}\")\n",
        "        print(f\"  y_slice shape in ndim 4 block: {y_slice.shape if y_slice is not None else 'None'}\")\n",
        "\n",
        "\n",
        "    elif x.ndim == 3:\n",
        "         print(\"Entering x.ndim == 3 block\")\n",
        "         img_slice = x[channel]\n",
        "         y_slice = y if y.ndim == 2 else y.squeeze()\n",
        "\n",
        "         try:\n",
        "             if pred.ndim > 2:\n",
        "                 pred_slice = pred.squeeze()\n",
        "                 print(f\"  Assigned pred_slice using pred.squeeze(). Shape: {pred_slice.shape if pred_slice is not None else 'None'}\") # Added print\n",
        "             else:\n",
        "                 pred_slice = pred\n",
        "                 print(f\"  Assigned pred_slice using pred. Shape: {pred_slice.shape if pred_slice is not None else 'None'}\") # Added print\n",
        "         except Exception as e:\n",
        "             print(f\"Error during pred_slice assignment in ndim 3 block: {e}\") # Added error print\n",
        "             pred_slice = None\n",
        "\n",
        "         slice_idx = slice_idx if slice_idx is not None else \"N/A (single slice)\"\n",
        "         print(f\"  channel: {channel}, img_slice shape: {img_slice.shape if img_slice is not None else 'None'}\")\n",
        "         print(f\"  y_slice shape in ndim 3 block: {y_slice.shape if y_slice is not None else 'None'}\")\n",
        "\n",
        "\n",
        "    elif x.ndim == 2 and y.ndim == 2 and pred.ndim == 2:\n",
        "         print(\"Entering x.ndim == 2 block\")\n",
        "         img_slice = x\n",
        "         y_slice = y\n",
        "         pred_slice = pred # Assuming pred is already a 2D slice\n",
        "\n",
        "         slice_idx = slice_idx if slice_idx is not None else \"N/A (single slice)\"\n",
        "         print(f\"  img_slice shape: {img_slice.shape if img_slice is not None else 'None'}\")\n",
        "         print(f\"  y_slice shape in ndim 2 block: {y_slice.shape if y_slice is not None else 'None'}\")\n",
        "         print(f\"  pred_slice shape in ndim 2 block: {pred_slice.shape if pred_slice is not None else 'None'}\") # Added print\n",
        "\n",
        "    else:\n",
        "        print(\"Entering else (unsupported shape) block\")\n",
        "        raise ValueError(f\"Unsupported input shapes for plotting after squeezing: x={x.shape}, y={y.shape}, pred={pred.shape}\")\n",
        "\n",
        "    # --- Explicit check for pred_slice here ---\n",
        "    print(f\"Value of pred_slice before comparison: {pred_slice}\") # Added print\n",
        "    if pred_slice is None:\n",
        "         raise UnboundLocalError(\"pred_slice is None after conditional assignments.\")\n",
        "    # --- End of explicit check ---\n",
        "\n",
        "\n",
        "    if img_slice is None or y_slice is None: # pred_slice is checked explicitly above\n",
        "         raise ValueError(\"img_slice or y_slice was not assigned in plot_sample_colored. Check input dimensions.\")\n",
        "\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    try:\n",
        "        img = img_slice\n",
        "        print(\"img assigned from img_slice.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during img = img_slice: {e}\")\n",
        "        raise\n",
        "\n",
        "    # --- Add these inspection prints for prediction slice ---\n",
        "    print(f\"Prediction slice values before thresholding: Unique values={torch.unique(pred_slice)}, min={pred_slice.min()}, max={pred_slice.max()}, mean={pred_slice.mean()}\")\n",
        "    # --- End of inspection prints ---\n",
        "\n",
        "\n",
        "    # --- Add this check and potential adjustment for mask values ---\n",
        "    print(f\"Ground Truth mask values before plotting: Unique values={torch.unique(y_slice)}, min={y_slice.min()}, max={y_slice.max()}\")\n",
        "    # Ensure ground truth mask is binary (0 or 1) and of float type for plotting\n",
        "    y_slice_display = (y_slice > 0).float()\n",
        "    print(f\"Ground Truth mask values after processing for display: Unique values={torch.unique(y_slice_display)}, min={y_slice_display.min()}, max={y_slice_display.max()}\")\n",
        "\n",
        "    print(f\"Prediction mask values before plotting: Unique values={torch.unique(pred_slice)}, min={pred_slice.min()}, max={pred_slice.max()}\") # This is a repeat, but useful to keep near binarization\n",
        "    # Ensure prediction mask is binary (0 or 1) and of float type for plotting\n",
        "    pred_slice_display = (pred_slice > 0.5).float() # Using 0.5 threshold for prediction\n",
        "    print(f\"Prediction mask values after processing for display: Unique values={torch.unique(pred_slice_display)}, min={pred_slice_display.min()}, max={pred_slice_display.max()}\")\n",
        "    # --- End of check and adjustment ---\n",
        "\n",
        "\n",
        "    try:\n",
        "        print(f\"Checking img dtype: {img.dtype}\")\n",
        "        img_dtype_np = img.dtype.numpy() if hasattr(img.dtype, 'numpy') else img.dtype\n",
        "        print(\"img_dtype_np created successfully.\")\n",
        "    except Exception as e:\n",
        "         print(f\"Error converting img dtype to numpy or during hasattr: {e}\")\n",
        "         raise\n",
        "\n",
        "    try:\n",
        "        print(f\"Checking img min/max: {img.min()}, {img.max()}\")\n",
        "        if not (np.issubdtype(img_dtype_np, np.floating) and img.min() >= 0 and img.max() <= 1) and \\\n",
        "           not (np.issubdtype(img_dtype_np, np.integer) and img.min() >= 0 and img.max() <= 255):\n",
        "           warnings.warn(\"Image data does not appear to be in expected range [0, 1] (float) or [0, 255] (uint8) for imshow. Normalization might be off or data has unexpected values.\")\n",
        "        print(\"Warning check passed or warning issued.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during warning check: {e}\")\n",
        "        pass\n",
        "\n",
        "\n",
        "    try:\n",
        "        print(\"Attempting to plot Input image.\")\n",
        "        img_display = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
        "        axes[0].imshow(img_display, cmap='gray')\n",
        "        axes[0].set_title(f\"Input (Channel {channel}) - Slice {slice_idx}\")\n",
        "        axes[0].axis('off')\n",
        "        print(\"Input image plotted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting Input image: {e}\")\n",
        "        raise\n",
        "\n",
        "    try:\n",
        "        print(\"Attempting to plot Ground Truth mask.\")\n",
        "        axes[1].imshow(img_display, cmap='gray')\n",
        "        axes[1].imshow(y_slice_display, cmap='Greens', alpha=0.5)\n",
        "        axes[1].set_title(f\"Ground Truth Mask - Slice {slice_idx}\")\n",
        "        axes[1].axis('off')\n",
        "        print(\"Ground Truth mask plotted.\")\n",
        "    except Exception as e:\n",
        "         print(f\"Error plotting Ground Truth mask: {e}\")\n",
        "         raise\n",
        "\n",
        "\n",
        "    try:\n",
        "        print(\"Attempting to plot Prediction mask.\")\n",
        "        axes[2].imshow(img_display, cmap='gray')\n",
        "        axes[2].imshow(pred_slice_display, cmap='Reds', alpha=0.5)\n",
        "        axes[2].set_title(f\"Prediction Mask (red) - Slice {slice_idx}\")\n",
        "        axes[2].axis('off')\n",
        "        print(\"Prediction mask plotted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting Prediction mask: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Iterate through the model_paths_dict to load models from all categories\n",
        "# This assumes model_paths_dict is populated from previous training steps.\n",
        "if 'model_paths_dict' not in globals() or not model_paths_dict:\n",
        "     print(\"Error: 'model_paths_dict' is not populated. Please ensure models were trained and saved.\")\n",
        "else:\n",
        "    for model_name, paths in model_paths_dict.items():\n",
        "        print(f\"Loading models for: {model_name}\")\n",
        "        if model_name not in model_architectures:\n",
        "             print(f\"Warning: Configuration for '{model_name}' not found in model_architectures. Cannot recreate model structure. Skipping.\")\n",
        "             continue\n",
        "\n",
        "        config = model_architectures[model_name]\n",
        "        arch_name = config[\"arch\"]\n",
        "        encoder_name = config[\"encoder_name\"]\n",
        "\n",
        "        for path in paths:\n",
        "            try:\n",
        "                # Recreate the model structure using the retrieved config\n",
        "                model = get_smp_model(\n",
        "                     arch=arch_name,\n",
        "                     encoder_name=encoder_name,\n",
        "                     in_channels=2, # DWI and ADC (assuming these are your input channels)\n",
        "                     out_classes=1  # Binary segmentation\n",
        "                ).to(DEVICE)\n",
        "                # Load the state dictionary\n",
        "                model.load_state_dict(torch.load(path, map_location=DEVICE))\n",
        "                model.eval() # Set model to evaluation mode\n",
        "                all_loaded_models.append(model)\n",
        "                print(f\"Loaded model from {path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading model from {path}: {e}\")\n",
        "                continue # Skip loading this model if there's an error\n",
        "\n",
        "print(f\"Total models loaded for ensemble prediction: {len(all_loaded_models)}\")\n",
        "\n",
        "# **Step 6: Ensemble Prediction and Visualization**\n",
        "# %%\n",
        "print(f\"# Ensemble prediction on a batch and visualization (using all loaded models)\")\n",
        "\n",
        "# Get a batch from the DataLoader for prediction\n",
        "try:\n",
        "    x_batch, y_batch = next(iter(train_loader)) # Use generic names as it's for combined ensemble\n",
        "except StopIteration:\n",
        "    print(\"No data in train_loader. Check dataset or batch size.\")\n",
        "    x_batch, y_batch = None, None\n",
        "\n",
        "if x_batch is not None and all_loaded_models: # Ensure data and models are available\n",
        "    print(\"Input batch min/max:\", x_batch.min(), x_batch.max())\n",
        "    print(\"Mask batch unique values:\", torch.unique(y_batch))\n",
        "\n",
        "    x_batch = x_batch.to(DEVICE, dtype=torch.float)\n",
        "    y_batch = y_batch.to(DEVICE, dtype=torch.float)\n",
        "\n",
        "    target_spatial_shape_3d = y_batch.shape[2:]\n",
        "\n",
        "    # Perform ensemble prediction using all loaded models\n",
        "    final_ensemble_mask, averaged_ensemble_probs = ensemble_predict_slice_by_slice(all_loaded_models, x_batch, target_spatial_shape_3d)\n",
        "\n",
        "    if final_ensemble_mask is not None:\n",
        "        sample_index_to_plot = 0 # Choose which sample in the batch to plot\n",
        "        dwi_channel = 0\n",
        "        adc_channel = 1\n",
        "\n",
        "        # --- Find a slice with a mask ---\n",
        "        slice_index_with_mask = None\n",
        "        # Assuming y_batch is [B, 1, D, H, W]\n",
        "        ground_truth_volume = y_batch[sample_index_to_plot, 0] # Get the 3D mask for the sample [D, H, W]\n",
        "\n",
        "        print(f\"Searching for a slice with a mask in sample {sample_index_to_plot}...\")\n",
        "        for d in range(ground_truth_volume.shape[0]): # Iterate through depth\n",
        "            mask_slice = ground_truth_volume[d]\n",
        "            if torch.sum(mask_slice) > 0: # Check if there are any non-zero pixels in the slice\n",
        "                slice_index_with_mask = d\n",
        "                print(f\"Found slice with mask at depth index: {slice_index_with_mask}\")\n",
        "                break # Stop searching once a slice with a mask is found\n",
        "\n",
        "        if slice_index_with_mask is not None:\n",
        "            slice_index_to_plot = slice_index_with_mask\n",
        "            print(f\"Plotting results for sample {sample_index_to_plot}, using slice with mask: {slice_index_to_plot}\")\n",
        "\n",
        "            # Plotting DWI input, GT, and Ensemble Prediction for the found slice\n",
        "            plot_sample_colored(\n",
        "                x_batch[sample_index_to_plot],\n",
        "                y_batch[sample_index_to_plot],\n",
        "                final_ensemble_mask[sample_index_to_plot],\n",
        "                channel=dwi_channel,\n",
        "                slice_idx=slice_index_to_plot\n",
        "            )\n",
        "\n",
        "            # Optionally, plot ADC input, GT, and Ensemble Prediction as well\n",
        "            plot_sample_colored(\n",
        "                x_batch[sample_index_to_plot],\n",
        "                y_batch[sample_index_to_plot],\n",
        "                final_ensemble_mask[sample_index_to_plot],\n",
        "                channel=adc_channel,\n",
        "                slice_idx=slice_index_to_plot\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            print(f\"No slices with a mask found in sample {sample_index_to_plot}. Cannot plot a slice with a visible ground truth mask.\")\n",
        "            # Optionally, plot a default slice anyway, but inform the user it has no mask\n",
        "            # slice_index_to_plot = final_ensemble_mask.shape[2] // 2 # Default to middle slice\n",
        "            # print(f\"No mask found in sample {sample_index_to_plot}. Plotting middle slice {slice_index_to_plot} anyway.\")\n",
        "            # plot_sample_colored(...) # Call plotting with the default slice\n",
        "        # --- End of finding slice with mask ---\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"Ensemble prediction failed.\")\n",
        "\n",
        "else:\n",
        "    if x_batch is None:\n",
        "         print(\"Skipping ensemble prediction as no data batch was loaded.\")\n",
        "    if not all_loaded_models:\n",
        "         print(\"Skipping ensemble prediction as no models were loaded.\")"
      ],
      "metadata": {
        "id": "_tKtY5BuECBS",
        "outputId": "fd94105c-e940-46f4-a0f7-ba9716c80161",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "_tKtY5BuECBS",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Loading All Trained Models from model_paths_dict for Ensemble Prediction\n",
            "model_paths_dict after assignment:\n",
            "{'unet_resnet18': ['/content/drive/My Drive/unet_resnet18_new_model_0.pth'], 'unetplusplus_resnet34': ['/content/drive/My Drive/unetplusplus_resnet34_new_model_0.pth'], 'deeplabv3plus_resnet50': ['/content/drive/My Drive/deeplabv3plus_resnet50_new_model_0.pth'], 'linknet_resnet18': ['/content/drive/My Drive/linknet_resnet18_new_model_0.pth']}\n",
            "entering 3D samples\n",
            "Total 3D samples: 250\n",
            "Loading models for: unet_resnet18\n",
            "Loaded model from /content/drive/My Drive/unet_resnet18_new_model_0.pth\n",
            "Loading models for: unetplusplus_resnet34\n",
            "Loaded model from /content/drive/My Drive/unetplusplus_resnet34_new_model_0.pth\n",
            "Loading models for: deeplabv3plus_resnet50\n",
            "Loaded model from /content/drive/My Drive/deeplabv3plus_resnet50_new_model_0.pth\n",
            "Loading models for: linknet_resnet18\n",
            "Loaded model from /content/drive/My Drive/linknet_resnet18_new_model_0.pth\n",
            "Total models loaded for ensemble prediction: 4\n",
            "# Ensemble prediction on a batch and visualization (using all loaded models)\n",
            "Input batch min/max: tensor(-606.4225) tensor(5265.8906)\n",
            "Mask batch unique values: tensor([0., 1.])\n",
            "Averaged probabilities min/max: 2.2898080113537583e-24 0.745600700378418\n",
            "Searching for a slice with a mask in sample 0...\n",
            "Found slice with mask at depth index: 32\n",
            "Plotting results for sample 0, using slice with mask: 32\n",
            "Inside plot_sample_colored function\n",
            "Original shapes: x=torch.Size([2, 112, 112, 73]), y=torch.Size([1, 112, 112, 73]), pred=torch.Size([1, 112, 112, 73])\n",
            "Shapes after squeezing: x=torch.Size([2, 112, 112, 73]), y=torch.Size([112, 112, 73]), pred=torch.Size([112, 112, 73])\n",
            "x.ndim after squeezing: 4\n",
            "pred.ndim after squeezing: 3\n",
            "slice_idx: 32\n",
            "Entering x.ndim == 4 block\n",
            "  Assigned pred_slice using pred[32]. Shape: torch.Size([112, 73])\n",
            "  slice_idx: 32, img_slice shape: torch.Size([112, 73])\n",
            "  y_slice shape in ndim 4 block: torch.Size([112, 73])\n",
            "Value of pred_slice before comparison: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "img assigned from img_slice.\n",
            "Prediction slice values before thresholding: Unique values=tensor([0.]), min=0.0, max=0.0, mean=0.0\n",
            "Ground Truth mask values before plotting: Unique values=tensor([0., 1.]), min=0.0, max=1.0\n",
            "Ground Truth mask values after processing for display: Unique values=tensor([0., 1.]), min=0.0, max=1.0\n",
            "Prediction mask values before plotting: Unique values=tensor([0.]), min=0.0, max=0.0\n",
            "Prediction mask values after processing for display: Unique values=tensor([0.]), min=0.0, max=0.0\n",
            "Checking img dtype: torch.float32\n",
            "img_dtype_np created successfully.\n",
            "Checking img min/max: -10.612001419067383, 1900.8770751953125\n",
            "Error during warning check: Cannot interpret 'torch.float32' as a data type\n",
            "Attempting to plot Input image.\n",
            "Input image plotted.\n",
            "Attempting to plot Ground Truth mask.\n",
            "Ground Truth mask plotted.\n",
            "Attempting to plot Prediction mask.\n",
            "Prediction mask plotted.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inside plot_sample_colored function\n",
            "Original shapes: x=torch.Size([2, 112, 112, 73]), y=torch.Size([1, 112, 112, 73]), pred=torch.Size([1, 112, 112, 73])\n",
            "Shapes after squeezing: x=torch.Size([2, 112, 112, 73]), y=torch.Size([112, 112, 73]), pred=torch.Size([112, 112, 73])\n",
            "x.ndim after squeezing: 4\n",
            "pred.ndim after squeezing: 3\n",
            "slice_idx: 32\n",
            "Entering x.ndim == 4 block\n",
            "  Assigned pred_slice using pred[32]. Shape: torch.Size([112, 73])\n",
            "  slice_idx: 32, img_slice shape: torch.Size([112, 73])\n",
            "  y_slice shape in ndim 4 block: torch.Size([112, 73])\n",
            "Value of pred_slice before comparison: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "img assigned from img_slice.\n",
            "Prediction slice values before thresholding: Unique values=tensor([0.]), min=0.0, max=0.0, mean=0.0\n",
            "Ground Truth mask values before plotting: Unique values=tensor([0., 1.]), min=0.0, max=1.0\n",
            "Ground Truth mask values after processing for display: Unique values=tensor([0., 1.]), min=0.0, max=1.0\n",
            "Prediction mask values before plotting: Unique values=tensor([0.]), min=0.0, max=0.0\n",
            "Prediction mask values after processing for display: Unique values=tensor([0.]), min=0.0, max=0.0\n",
            "Checking img dtype: torch.float32\n",
            "img_dtype_np created successfully.\n",
            "Checking img min/max: -332.8429870605469, 4030.112060546875\n",
            "Error during warning check: Cannot interpret 'torch.float32' as a data type\n",
            "Attempting to plot Input image.\n",
            "Input image plotted.\n",
            "Attempting to plot Ground Truth mask.\n",
            "Ground Truth mask plotted.\n",
            "Attempting to plot Prediction mask.\n",
            "Prediction mask plotted.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Boosting Ensemble Training (Simplified Sequential)\n",
        "print(f\"#Training Boosting Ensemble Base Models (Simplified Sequential)\")\n",
        "\n",
        "boosting_models = []\n",
        "N_BOOSTING_MODELS = 3 # Number of models in the boosting sequence\n",
        "BOOSTING_MODEL_CONFIG = \"unet_resnet18_config1\" # Use one specific configuration for boosting\n",
        "\n",
        "# You will need a way to calculate sample/pixel weights based on previous model errors.\n",
        "# This is a conceptual placeholder.\n",
        "def calculate_error_weights(ground_truth, previous_model_predictions):\n",
        "    # Implement logic to calculate weights based on where the previous model\n",
        "    # made errors or was uncertain.\n",
        "    # Example: weights could be higher for misclassified pixels.\n",
        "    # This would require running inference on the training data after each boosting step.\n",
        "    return torch.ones_like(ground_truth).to(ground_truth.device) # Placeholder: uniform weights\n",
        "\n",
        "\n",
        "# Define the model configurations for the boosting ensemble\n",
        "model_configurations = {\n",
        "    \"unet_resnet18\": {\"arch\": \"unet\", \"encoder_name\": \"resnet18\"},\n",
        "    \"deeplabv3plus_resnet50\": {\"arch\": \"deeplabv3plus\", \"encoder_name\": \"resnet50\"},\n",
        "    \"unetplusplus_resnet34\": {\"arch\": \"unetplusplus\", \"encoder_name\": \"resnet34\"},\n",
        "    \"linknet_resnet18\": {\"arch\": \"linknet\", \"encoder_name\": \"resnet18\"}\n",
        "}\n",
        "\n",
        "# Choose which configuration to use for the boosting models\n",
        "BOOSTING_MODEL_CONFIG = \"unet_resnet18\" # Example: Use the UNet with ResNet18 encoder\n",
        "\n",
        "# --- Your boosting training loop starts here ---\n",
        "# ...\n",
        "for i in range(N_BOOSTING_MODELS):\n",
        "    print(f\"Training Boosting model {i+1}/{N_BOOSTING_MODELS}\")\n",
        "\n",
        "    # Get model (using smp helper or custom 3D model) - use the same architecture\n",
        "    config = model_configurations[BOOSTING_MODEL_CONFIG]\n",
        "    # ... rest of your boosting code ...\n",
        "\n",
        "\n",
        "for i in range(N_BOOSTING_MODELS):\n",
        "    print(f\"  Training Boosting model {i+1}/{N_BOOSTING_MODELS}\")\n",
        "\n",
        "    # Get model (using smp helper or custom 3D model) - use the same architecture\n",
        "    config = model_configurations[BOOSTING_MODEL_CONFIG]\n",
        "    try:\n",
        "         if config[\"arch\"] in [\"unet\", \"unetplusplus\", \"deeplabv3plus\", \"linknet\"]: # smp models\n",
        "             model = get_smp_model(arch=config[\"arch\"], encoder_name=config[\"encoder_name\"], in_channels=2, out_classes=1).to(DEVICE)\n",
        "         # elif config[\"arch\"] == \"unet3d\": # Custom 3D model\n",
        "         #      model = UNet3D(...).to(DEVICE) # Initialize your 3D model\n",
        "         else:\n",
        "              raise ValueError(f\"Unknown architecture: {config['arch']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating boosting model {i+1}: {e}\")\n",
        "        continue # Skip to the next model instance\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = dice_loss # You might use a weighted loss\n",
        "\n",
        "    # --- Training Loop (Slice-by-Slice if using smp) ---\n",
        "    # This will be similar to the bagging training loop, but you'll need\n",
        "    # to incorporate error weights if you implement weighted training.\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_idx, (x_3d, y_3d) in enumerate(train_loader):\n",
        "             x_3d = x_3d.to(DEVICE, dtype=torch.float)\n",
        "             y_3d = y_3d.to(DEVICE, dtype=torch.float)\n",
        "\n",
        "             # Calculate error weights based on the previous model's predictions (if i > 0)\n",
        "             # This part is complex and requires running inference on the training data\n",
        "             # with the previously trained model.\n",
        "             # For this simplified example, we'll use uniform weights for now.\n",
        "             error_weights = calculate_error_weights(y_3d, None) # Placeholder\n",
        "\n",
        "\n",
        "             # --- Slice-by-Slice Processing (if using smp) ---\n",
        "             batch_losses = []\n",
        "             for d in range(x_3d.shape[2]):\n",
        "                 x_slice = x_3d[:, :, d, :, :].clone() # Get slice\n",
        "                 y_slice = y_3d[:, :, d, :, :].clone() # Get slice\n",
        "                 weights_slice = error_weights[:, :, d, :, :].clone() # Get corresponding weights slice\n",
        "\n",
        "\n",
        "                 # Padding for divisibility\n",
        "                 required_divisor = 32\n",
        "                 h, w = x_slice.shape[2:]\n",
        "                 new_h = (h + required_divisor - 1) // required_divisor * required_divisor\n",
        "                 new_w = (w + required_divisor - 1) // required_divisor * required_divisor\n",
        "                 target_padded_shape_slice = (new_h, new_w)\n",
        "\n",
        "                 x_slice_padded = pad_or_crop_to_shape_2d(x_slice, target_padded_shape_slice)\n",
        "                 y_slice_padded = pad_or_crop_to_shape_2d(y_slice, target_padded_shape_slice)\n",
        "                 weights_slice_padded = pad_or_crop_to_shape_2d(weights_slice, target_padded_shape_slice)\n",
        "\n",
        "\n",
        "                 optimizer.zero_grad()\n",
        "                 out_slice = model(x_slice_padded)\n",
        "\n",
        "                 # Align output for loss\n",
        "                 target_spatial_shape_slice_for_loss = y_slice.shape[2:]\n",
        "                 out_slice_aligned_for_loss = pad_or_crop_to_shape_2d(out_slice, target_spatial_shape_slice_for_loss)\n",
        "\n",
        "                 # Use a weighted loss function\n",
        "                 # You would need to implement a weighted Dice Loss or similar\n",
        "                 # For this example, using regular Dice Loss\n",
        "                 loss = criterion(out_slice_aligned_for_loss, y_slice) # Use y_slice for loss\n",
        "                 # loss = weighted_dice_loss(out_slice_aligned_for_loss, y_slice_padded, weights_slice_padded) # If using weighted loss\n",
        "\n",
        "\n",
        "                 loss.backward()\n",
        "                 batch_losses.append(loss.item())\n",
        "\n",
        "             # --- End of Slice-by-Slice Processing ---\n",
        "\n",
        "             # If using a 3D model:\n",
        "             # error_weights_3d = calculate_error_weights(y_3d, previous_model_predictions_3d)\n",
        "             # optimizer.zero_grad()\n",
        "             # out_3d = model(x_3d)\n",
        "             # loss = weighted_dice_loss_3d(out_3d, y_3d, error_weights_3d) # Need a 3D weighted loss\n",
        "             # loss.backward()\n",
        "\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += np.mean(batch_losses)\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f\"    Batch {batch_idx}/{len(train_loader)} Avg Slice Loss: {np.mean(batch_losses):.4f}\")\n",
        "\n",
        "    print(f\"  Epoch {epoch+1} Avg Slice Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # After training, save the boosting model instance\n",
        "    save_path = f\"boosting_{BOOSTING_MODEL_CONFIG.replace(' ', '_')}_instance_{i}.pth\"\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    boosting_models.append({\"path\": save_path, \"config_name\": BOOSTING_MODEL_CONFIG}) # Store path and config name\n",
        "    print(f\"  Saved {save_path}\")\n",
        "\n",
        "    # Important for boosting: After training a model, you need to run inference\n",
        "    # on the *entire* training dataset with this model to calculate the error weights\n",
        "    # for the *next* boosting model. This requires another inference loop over the\n",
        "    # training data, which can be time-consuming.\n",
        "    # For this conceptual example, we are skipping the actual error weight calculation.\n",
        "\n"
      ],
      "metadata": {
        "id": "BGM3vm4p68Jd"
      },
      "id": "BGM3vm4p68Jd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# %% Boosting Ensemble Prediction\n",
        "print(f\"#Boosting Ensemble Prediction\")\n",
        "\n",
        "# Boosting ensemble prediction typically involves a weighted sum of the base model predictions.\n",
        "# The weights for each base model are usually determined during the boosting training process\n",
        "# (e.g., based on their performance).\n",
        "\n",
        "# For simplicity, we can illustrate with simple averaging or equal weighting for now.\n",
        "# A more proper boosting prediction would use learned weights.\n",
        "\n",
        "# Load boosting models from paths\n",
        "loaded_boosting_models = []\n",
        "for model_info in boosting_models:\n",
        "     path = model_info[\"path\"]\n",
        "     config_name = model_info[\"config_name\"]\n",
        "\n",
        "     try:\n",
        "         # Find the original configuration from model_configurations\n",
        "         original_config = model_configurations.get(config_name)\n",
        "\n",
        "         if original_config:\n",
        "              if original_config[\"arch\"] in [\"unet\", \"unetplusplus\", \"deeplabv3plus\", \"linknet\"]: # smp models\n",
        "                 model = get_smp_model(arch=original_config[\"arch\"], encoder_name=original_config[\"encoder_name\"], in_channels=2, out_classes=1).to(DEVICE)\n",
        "             # elif original_config[\"arch\"] == \"unet3d\": # Custom 3D model\n",
        "             #      model = UNet3D(...).to(DEVICE)\n",
        "              else:\n",
        "                   print(f\"Unknown architecture in saved path: {original_config['arch']}\")\n",
        "                   continue\n",
        "\n",
        "              model.load_state_dict(torch.load(path, map_location=DEVICE))\n",
        "              model.eval()\n",
        "              loaded_boosting_models.append(model)\n",
        "              print(f\"  Loaded boosting model from {path}\")\n",
        "         else:\n",
        "             print(f\"  Could not find configuration for saved model path: {path}\")\n",
        "             continue\n",
        "\n",
        "     except Exception as e:\n",
        "         print(f\"  Error loading boosting model from {path}: {e}\")\n",
        "         continue\n",
        "\n",
        "print(f\"Number of loaded boosting models: {len(loaded_boosting_models)}\")\n",
        "\n",
        "# Get a batch from the DataLoader for prediction\n",
        "# Using train_loader here for demonstration, replace with a validation/test loader\n",
        "try:\n",
        "    x_batch_bagging, y_batch_bagging = next(iter(train_loader))\n",
        "except StopIteration:\n",
        "    print(\"No data in train_loader. Check dataset or batch size.\")\n",
        "    x_batch_bagging, y_batch_bagging = None, None # Handle empty loader\n",
        "\n",
        "if x_batch_bagging is not None:\n",
        "    print(\"Input batch min/max:\", x_batch_bagging.min(), x_batch_bagging.max())\n",
        "    print(\"Mask batch unique values:\", torch.unique(y_batch_bagging))\n",
        "\n",
        "    # Ensure correct dims and type for prediction and move to device\n",
        "    x_batch_bagging = x_batch_bagging.to(DEVICE, dtype=torch.float)\n",
        "    y_batch_bagging = y_batch_bagging.to(DEVICE, dtype=torch.float) # Also move target mask to device\n",
        "\n",
        "    # Define the target shape for alignment - this should match the shape expected by your padding/cropping functions\n",
        "    # You might need to determine this based on your data and pad_collate output\n",
        "    target_spatial_shape_3d_bagging = y_batch_bagging.shape[2:] # Example: Assuming y_batch_bagging is [B, 1, D, H, W]\n",
        "\n",
        "    # --- Your boosting ensemble prediction code starts here ---\n",
        "    print(f\"# Boosting Ensemble prediction on a batch\")\n",
        "\n",
        "    # Use ensemble_predict_slice_by_slice (or a similar function) for prediction\n",
        "    # This will average the predictions of the loaded boosting models.\n",
        "    final_boosting_mask, averaged_boosting_probs = ensemble_predict_slice_by_slice(loaded_boosting_models, x_batch_bagging, target_spatial_shape_3d_bagging)\n",
        "    # ... rest of your boosting evaluation code ...\n",
        "else:\n",
        "    print(\"Skipping boosting ensemble prediction as no data batch was loaded.\")\n",
        "\n",
        "if len(loaded_boosting_models) > 0:\n",
        "    # Use ensemble_predict_slice_by_slice (or a similar function) for prediction\n",
        "    # This will average the predictions of the loaded boosting models.\n",
        "    final_boosting_mask, averaged_boosting_probs = ensemble_predict_slice_by_slice(loaded_boosting_models, x_batch_bagging, target_spatial_shape_3d_bagging) # Reuse batch from bagging\n",
        "\n",
        "    # Evaluate Boosting Ensemble prediction\n",
        "    boosting_dice = dice_score(final_boosting_mask, safe_unsqueeze_mask(y_batch_bagging.to(DEVICE)))\n",
        "    print(f\"Boosting Ensemble Dice Score: {boosting_dice:.4f}\")\n",
        "\n",
        "    # Plot Boosting Ensemble prediction for a sample\n",
        "    plot_sample_colored(x_batch_bagging[0], y_batch_bagging[0], final_boosting_mask[0], channel=0)\n",
        "    plot_sample_separate_modalities(x_batch_bagging[0], y_batch_bagging[0], final_boosting_mask[0])\n",
        "\n",
        "else:\n",
        "     print(\"No boosting models were loaded for prediction.\")"
      ],
      "metadata": {
        "id": "8Oyt0P03Y_Kh"
      },
      "id": "8Oyt0P03Y_Kh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A more common and practical approach for boosting with neural networks is Gradient Boosting, often implemented using frameworks like LightGBM or XGBoost. However, these are usually applied to tabular data or features extracted from images, not directly to the raw image data within the neural network training loop in the same way you're training your UNet/VNet.\n",
        "\n",
        "Implementing true boosting directly with your existing PyTorch models would be quite complex. A simplified approach that captures the spirit of boosting with neural networks might involve:\n",
        "\n",
        "Train a base model.\n",
        "Identify pixels where the base model made incorrect predictions.\n",
        "Train a second model, giving higher weight or focus to the misclassified pixels from the first model's predictions.\n",
        "Combine the predictions of the models, often with weights.\n",
        "This is still a significant departure from your current training loop.\n",
        "\n",
        "Given the context of your existing code and the complexity of true boosting with deep learning architectures, a more feasible approach that aligns with your current structure would be to adapt a method that mimics some aspects of boosting or sequential training, but within the framework of your existing models.\n",
        "\n",
        "One way to do this is to train models sequentially, and in each subsequent training phase, somehow influence the training data or loss function based on the errors of the previous model.\n",
        "\n",
        "Here's a conceptual outline and some code snippets to illustrate this simplified \"sequential error focus\" approach using your existing UNet3D model. This is NOT a true AdaBoost or Gradient Boosting implementation but aims to show a sequential training process where later models pay more attention to areas where earlier models struggled.\n",
        "\n",
        "Conceptual Steps:\n",
        "\n",
        "Train the first UNet3D model on the full dataset.\n",
        "After training the first model, evaluate its performance on the training data to identify pixels where it performed poorly (e.g., where the predicted probability was close to 0.5 for the incorrect class).\n",
        "Create a weighting mask based on these errors. Pixels where the first model was uncertain or wrong get higher weights.\n",
        "Train a second UNet3D model, incorporating this weighting mask into the loss function (e.g., using a weighted Dice Loss). This encourages the second model to focus more on the challenging pixels.\n",
        "Optionally, repeat for more models.\n",
        "Combine the predictions of the models (e.g., weighted averaging)."
      ],
      "metadata": {
        "id": "BXSpvJgdAExs"
      },
      "id": "BXSpvJgdAExs"
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Stacking Ensemble Training - Base Models\n",
        "print(f\"#Training Stacking Ensemble Base Models\")\n",
        "\n",
        "stacking_base_models = {}\n",
        "N_STACKING_BASE_MODELS = 1 # Train one instance of each configuration for stacking\n",
        "\n",
        "for model_name, config in model_configurations.items():\n",
        "    stacking_base_models[model_name] = []\n",
        "    print(f\"  Training Stacking base model {model_name}\")\n",
        "\n",
        "    # Get model\n",
        "    try:\n",
        "         if config[\"arch\"] in [\"unet\", \"unetplusplus\", \"deeplabv3plus\", \"linknet\"]: # smp models\n",
        "             model = get_smp_model(arch=config[\"arch\"], encoder_name=config[\"encoder_name\"], in_channels=2, out_classes=1).to(DEVICE)\n",
        "         # elif config[\"arch\"] == \"unet3d\": # Custom 3D model\n",
        "         #      model = UNet3D(...).to(DEVICE)\n",
        "         else:\n",
        "              raise ValueError(f\"Unknown architecture: {config['arch']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating stacking base model {model_name}: {e}\")\n",
        "        continue # Skip to the next model\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = dice_loss\n",
        "\n",
        "    # --- Training Loop (Similar to Bagging/Boosting) ---\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_idx, (x_3d, y_3d) in enumerate(train_loader):\n",
        "             x_3d = x_3d.to(DEVICE, dtype=torch.float)\n",
        "             y_3d = y_3d.to(DEVICE, dtype=torch.float)\n",
        "\n",
        "             # --- Slice-by-Slice Processing (if using smp) ---\n",
        "             batch_losses = []\n",
        "             for d in range(x_3d.shape[2]):\n",
        "                 x_slice = x_3d[:, :, d, :, :].clone()\n",
        "                 y_slice = y_3d[:, :, d, :, :].clone()\n",
        "\n",
        "                 # Padding for divisibility\n",
        "                 required_divisor = 32\n",
        "                 h, w = x_slice.shape[2:]\n",
        "                 new_h = (h + required_divisor - 1) // required_divisor * required_divisor\n",
        "                 new_w = (w + required_divisor - 1) // required_divisor * required_divisor\n",
        "                 target_padded_shape_slice = (new_h, new_w)\n",
        "\n",
        "                 x_slice_padded = pad_or_crop_to_shape_2d(x_slice, target_padded_shape_slice)\n",
        "                 y_slice_padded = pad_or_crop_to_shape_2d(y_slice, target_padded_shape_slice)\n",
        "\n",
        "\n",
        "                 optimizer.zero_grad()\n",
        "                 out_slice = model(x_slice_padded)\n",
        "\n",
        "                 # Align output for loss\n",
        "                 target_spatial_shape_slice_for_loss = y_slice.shape[2:]\n",
        "                 out_slice_aligned_for_loss = pad_or_crop_to_shape_2d(out_slice, target_spatial_shape_slice_for_loss)\n",
        "\n",
        "                 loss = criterion(out_slice_aligned_for_loss, y_slice)\n",
        "\n",
        "                 loss.backward()\n",
        "                 batch_losses.append(loss.item())\n",
        "\n",
        "             # --- End of Slice-by-Slice Processing ---\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += np.mean(batch_losses)\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"    Batch {batch_idx}/{len(train_loader)} Avg Slice Loss: {np.mean(batch_losses):.4f}\")\n",
        "\n",
        "        print(f\"  Epoch {epoch+1} Avg Slice Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # Save the trained base model\n",
        "    save_path = f\"stacking_base_{model_name.replace(' ', '_')}.pth\"\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    stacking_base_models[model_name].append(save_path)\n",
        "    print(f\"  Saved {save_path}\")\n",
        "\n",
        "\n",
        "# %% Stacking Ensemble - Generate Meta-Features on Validation Data\n",
        "print(f\"#Generating Meta-Features for Stacking\")\n",
        "\n",
        "# For stacking, you need a separate validation dataset.\n",
        "# This is crucial to prevent data leakage.\n",
        "# Assume you have a validation loader: val_loader\n",
        "\n",
        "# Load trained base models\n",
        "loaded_stacking_base_models = {}\n",
        "for model_name, paths in stacking_base_models.items():\n",
        "    if paths: # Assuming only one instance per config for stacking base\n",
        "        path = paths[0]\n",
        "        try:\n",
        "            config = model_configurations[model_name]\n",
        "            if config[\"arch\"] in [\"unet\", \"unetplusplus\", \"deeplabv3plus\", \"linknet\"]:\n",
        "                 model = get_smp_model(arch=config[\"arch\"], encoder_name=config[\"encoder_name\"], in_channels=2, out_classes=1).to(DEVICE)\n",
        "            # elif config[\"arch\"] == \"unet3d\": model = UNet3D(...).to(DEVICE)\n",
        "            else:\n",
        "                print(f\"Unknown architecture for stacking base model: {config['arch']}\")\n",
        "                continue\n",
        "\n",
        "            model.load_state_dict(torch.load(path, map_location=DEVICE))\n",
        "            model.eval()\n",
        "            loaded_stacking_base_models[model_name] = model\n",
        "            print(f\"  Loaded stacking base model from {path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Error loading stacking base model from {path}: {e}\")\n",
        "            continue\n",
        "    else:\n",
        "        print(f\"  No saved model found for stacking base config: {model_name}\")\n",
        "\n",
        "\n",
        "# Collect predictions from base models on the validation set\n",
        "# These predictions will be the meta-features for the meta-model.\n",
        "# The shape of meta-features will be [num_validation_samples, num_base_models, D, H, W]\n",
        "# (or D*H*W for a flattened approach).\n",
        "\n",
        "all_base_model_val_preds = []\n",
        "all_val_masks = [] # Collect validation masks to train the meta-model\n",
        "\n",
        "# Assuming val_loader is defined and contains validation data\n",
        "# for val_batch_idx, (x_val_3d, y_val_3d) in enumerate(val_loader):\n",
        "#     x_val_3d = x_val_3d.to(DEVICE, dtype=torch.float)\n",
        "#     y_val_3d = y_val_3d.to(DEVICE, dtype=torch.float) # Keep on device for now\n",
        "\n",
        "#     batch_base_model_preds = []\n",
        "#     for model_name, model in loaded_stacking_base_models.items():\n",
        "#         print(f\"    Generating predictions for stacking base model: {model_name}\")\n",
        "#         with torch.no_grad():\n",
        "#             # Predict slice by slice (if using smp)\n",
        "#             model_val_predictions = []\n",
        "#             for b in range(x_val_3d.shape[0]):\n",
        "#                  volume_predictions = []\n",
        "#                  for d in range(x_val_3d.shape[2]):\n",
        "#                      x_slice = x_val_3d[b:b+1, :, d:d+1, :, :].to(model.device, dtype=torch.float)\n",
        "#                      x_slice = x_slice.squeeze(2)\n",
        "#\n",
        "#                      # Padding for divisibility\n",
        "#                      required_divisor = 32\n",
        "#                      h, w = x_slice.shape[2:]\n",
        "#                      new_h = (h + required_divisor - 1) // required_divisor * required_divisor\n",
        "#                      new_w = (w + required_divisor - 1) // required_divisor * required_divisor\n",
        "#                      target_padded_shape_slice = (new_h, new_w)\n",
        "#                      x_slice_padded = pad_or_crop_to_shape_2d(x_slice, target_padded_shape_slice)\n",
        "#\n",
        "#                      out_slice = model(x_slice_padded) # Get probability output\n",
        "#\n",
        "#                      # Align output\n",
        "#                      target_spatial_shape_slice_for_alignment = y_val_3d.shape[2:] # Should be from y_val_3d\n",
        "#                      out_slice_aligned = pad_or_crop_to_shape_2d(out_slice, target_spatial_shape_slice_for_alignment)\n",
        "#\n",
        "#                      out_slice_aligned = out_slice_aligned.unsqueeze(2) # Add depth dim back\n",
        "#                      volume_predictions.append(out_slice_aligned)\n",
        "#\n",
        "#                  volume_predictions = torch.cat(volume_predictions, dim=2)\n",
        "#                  model_val_predictions.append(volume_predictions)\n",
        "#\n",
        "#             model_val_predictions_batch = torch.cat(model_val_predictions, dim=0) # Predictions for the batch\n",
        "#             batch_base_model_preds.append(model_val_predictions_batch)\n",
        "#\n",
        "#     # Stack predictions from different base models for this batch\n",
        "#     # Shape: [batch_size, num_base_models, 1, D, H, W]\n",
        "#     stacked_batch_preds = torch.stack(batch_base_model_preds, dim=1) # Stack along dim 1\n",
        "#     all_base_model_val_preds.append(stacked_batch_preds)\n",
        "#     all_val_masks.append(y_val_3d)\n",
        "#\n",
        "# # Concatenate all batches to get the full meta-features and masks for the validation set\n",
        "# meta_features = torch.cat(all_base_model_val_preds, dim=0) # Shape: [total_val_samples, num_base_models, 1, D, H, W]\n",
        "# meta_masks = torch.cat(all_val_masks, dim=0) # Shape: [total_val_samples, 1, D, H, W]\n",
        "#\n",
        "# # Reshape meta_features for the meta-model if needed (e.g., flatten spatial dimensions)\n",
        "# # meta_features_flat = meta_features.view(meta_features.size(0), meta_features.size(1), -1) # Shape: [total_val_samples, num_base_models, D*H*W]\n",
        "# # meta_masks_flat = meta_masks.view(meta_masks.size(0), -1) # Shape: [total_val_samples, D*H*W]\n",
        "\n",
        "\n",
        "# %% Stacking Ensemble - Train Meta-Model\n",
        "print(f\"#Training Stacking Ensemble Meta-Model\")\n",
        "\n",
        "# The meta-model can be a simple model (e.g., a logistic regression, a small neural network,\n",
        "# or even another segmentation model).\n",
        "\n",
        "# Example: A simple 3D convolutional meta-model\n",
        "# class MetaModel3D(nn.Module):\n",
        "#     def __init__(self, in_channels=len(loaded_stacking_base_models), out_channels=1):\n",
        "#         super().__init__()\n",
        "#         # Example: a few conv layers\n",
        "#         self.conv1 = nn.Conv3d(in_channels, 32, kernel_size=3, padding=1)\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "#         self.conv2 = nn.Conv3d(32, out_channels, kernel_size=1) # Final output layer\n",
        "#\n",
        "#     def forward(self, x):\n",
        "#         # x will have shape [B, num_base_models, 1, D, H, W] or similar\n",
        "#         # Squeeze the channel dim if needed, or adapt conv1\n",
        "#         x = x.squeeze"
      ],
      "metadata": {
        "id": "w5Fx5UYQ_0GS"
      },
      "id": "w5Fx5UYQ_0GS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Evaluate individual ensemble models\n",
        "print(f\"#Evaluate individual ensemble models\")\n",
        "\n",
        "# Get a batch from the DataLoader for evaluation\n",
        "# Using train_loader here, ideally use a validation or test loader\n",
        "try:\n",
        "    x_batch_eval, y_batch_eval = next(iter(train_loader))\n",
        "except StopIteration:\n",
        "    print(\"No data in train_loader for individual model evaluation. Check dataset or batch size.\")\n",
        "    x_batch_eval, y_batch_eval = None, None # Handle empty loader\n",
        "\n",
        "# Use a list to store metrics for each model\n",
        "model_metrics_list = []\n",
        "\n",
        "if x_batch_eval is not None and len(ensemble_models) > 0:\n",
        "    x_batch_eval = x_batch_eval.to(DEVICE, dtype=torch.float)\n",
        "    y_batch_eval = y_batch_eval.to(DEVICE, dtype=torch.float)\n",
        "\n",
        "    # Determine the target shape for alignment - use the spatial shape of y\n",
        "    target_spatial_shape_3d_eval = y_batch_eval.shape[2:]\n",
        "\n",
        "    print(f\"Evaluating {len(ensemble_models)} individual models on a batch.\")\n",
        "\n",
        "    for i, model in enumerate(ensemble_models):\n",
        "        # You might need to get the original model name from model_paths_dict\n",
        "        # This requires matching the model instance back to its name/path\n",
        "        # For simplicity, let's use a generic name for now.\n",
        "        model_name = f\"Model {i+1}\"\n",
        "        # A more robust way would involve storing model names with instances during loading.\n",
        "\n",
        "        print(f\"  Evaluating {model_name}...\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Perform prediction for the individual model slice by slice\n",
        "            model_predictions = []\n",
        "            for b in range(x_batch_eval.shape[0]): # Iterate through batch\n",
        "                 volume_predictions = []\n",
        "                 for d in range(x_batch_eval.shape[2]): # Iterate through depth dimension\n",
        "                     x_slice = x_batch_eval[b:b+1, :, d:d+1, :, :].to(model.device, dtype=torch.float)\n",
        "                     x_slice = x_slice.squeeze(2) # Remove depth dim to get [1, C, H, W]\n",
        "\n",
        "                     # --- Add padding to make height and width divisible by the required divisor ---\n",
        "                     required_divisor = 32 # Use the largest required divisor observed\n",
        "                     h, w = x_slice.shape[2:]\n",
        "                     new_h = (h + required_divisor - 1) // required_divisor * required_divisor\n",
        "                     new_w = (w + required_divisor - 1) // required_divisor * required_divisor\n",
        "                     target_padded_shape_slice = (new_h, new_w)\n",
        "\n",
        "                     x_slice_padded = pad_or_crop_to_shape_2d(x_slice, target_padded_shape_slice)\n",
        "\n",
        "                     out_slice = model(x_slice_padded) # Model predicts probabilities for the slice [1, 1, H, W]\n",
        "\n",
        "                     # Ensure output slice is 4D [1, C, H, W] and align spatial dimensions\n",
        "                     while out_slice.ndim < 4:\n",
        "                         out_slice = out_slice.unsqueeze(0)\n",
        "\n",
        "                     # Align the model's output to the original target spatial shape\n",
        "                     target_spatial_shape_slice_for_alignment = target_spatial_shape_3d_eval[1:] # (H, W)\n",
        "                     out_slice_aligned = pad_or_crop_to_shape_2d(out_slice, target_spatial_shape_slice_for_alignment)\n",
        "\n",
        "                     # Threshold the prediction to get a binary mask for the slice\n",
        "                     pred_slice_binary = (out_slice_aligned > 0.5).float()\n",
        "\n",
        "                     # Add depth dimension back\n",
        "                     pred_slice_binary = pred_slice_binary.unsqueeze(2) # [1, 1, 1, H, W]\n",
        "                     volume_predictions.append(pred_slice_binary)\n",
        "\n",
        "                 # Stack slices back to form a 3D volume prediction for the sample\n",
        "                 volume_predictions = torch.cat(volume_predictions, dim=2) # Concatenate along depth [1, 1, D, H, W]\n",
        "                 model_predictions.append(volume_predictions)\n",
        "\n",
        "            # Concatenate predictions for the batch\n",
        "            model_predictions_batch = torch.cat(model_predictions, dim=0) # [B, 1, D, H, W]\n",
        "\n",
        "        # Calculate evaluation metrics for the batch prediction\n",
        "        # Ensure target mask has the correct shape [B, 1, D, H, W]\n",
        "        y_batch_eval_aligned = safe_unsqueeze_mask(y_batch_eval)\n",
        "\n",
        "        # Ensure both tensors have the same spatial dimensions for metric calculation\n",
        "        # This step is similar to the loss calculation alignment.\n",
        "        min_spatial_shape_eval = [min(a, b) for a, b in zip(model_predictions_batch.shape[2:], y_batch_eval_aligned.shape[2:])]\n",
        "        model_predictions_batch_aligned = model_predictions_batch[:, :, :min_spatial_shape_eval[0], :min_spatial_shape_eval[1], :min_spatial_shape_eval[2]]\n",
        "        y_batch_eval_aligned_cropped = y_batch_eval_aligned[:, :, :min_spatial_shape_eval[0], :min_spatial_shape_eval[1], :min_spatial_shape_eval[2]]\n",
        "\n",
        "\n",
        "        dice_batch = dice_score(model_predictions_batch_aligned, y_batch_eval_aligned_cropped)\n",
        "        # You can add AVD and F1 here if you have those functions\n",
        "        # avd_batch = absolute_volume_difference(model_predictions_batch_aligned.cpu().numpy(), y_batch_eval_aligned_cropped.cpu().numpy())\n",
        "        # f1_batch = lesion_wise_f1_score(model_predictions_batch_aligned.cpu().numpy(), y_batch_eval_aligned_cropped.cpu().numpy())\n",
        "\n",
        "        # Store the metrics in a dictionary for this model\n",
        "        metrics_entry = {\n",
        "            \"Model\": model_name,\n",
        "            \"Dice Score (Batch)\": dice_batch.item() # Use .item() to get scalar from tensor\n",
        "            # Add other metrics here\n",
        "            # \"AVD (Batch)\": avd_batch,\n",
        "            # \"Lesion-wise F1 (Batch)\": f1_batch\n",
        "        }\n",
        "        model_metrics_list.append(metrics_entry)\n",
        "\n",
        "# %% Display Metrics Table\n",
        "import pandas as pd\n",
        "\n",
        "if model_metrics_list:\n",
        "    df_metrics = pd.DataFrame(model_metrics_list)\n",
        "    print(\"\\nIndividual Model Evaluation Metrics (on a batch):\")\n",
        "    display(df_metrics) # Use display for a nice table in Colab\n",
        "\n",
        "# %% Rest of your code (Ensemble prediction, visualization)\n",
        "# ... (Your code for ensemble prediction and plotting) ...2"
      ],
      "metadata": {
        "id": "vPSRC0eT_svq"
      },
      "id": "vPSRC0eT_svq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Evaluate individual ensemble models\n",
        "print(f\"#Evaluate individual ensemble models\")\n",
        "\n",
        "# Get a batch from the DataLoader for evaluation\n",
        "# Using train_loader here, ideally use a validation or test loader\n",
        "try:\n",
        "    x_batch_eval, y_batch_eval = next(iter(train_loader))\n",
        "except StopIteration:\n",
        "    print(\"No data in train_loader for individual model evaluation. Check dataset or batch size.\")\n",
        "    x_batch_eval, y_batch_eval = None, None # Handle empty loader\n",
        "\n",
        "# Use a list to store metrics for each model\n",
        "model_metrics_list = []\n",
        "\n",
        "if x_batch_eval is not None and len(ensemble_models) > 0:\n",
        "    x_batch_eval = x_batch_eval.to(DEVICE, dtype=torch.float)\n",
        "    y_batch_eval = y_batch_eval.to(DEVICE, dtype=torch.float)\n",
        "\n",
        "    # Determine the target shape for alignment - use the spatial shape of y\n",
        "    target_spatial_shape_3d_eval = y_batch_eval.shape[2:]\n",
        "\n",
        "    print(f\"Evaluating {len(ensemble_models)} individual models on a batch.\")\n",
        "\n",
        "    for i, model in enumerate(ensemble_models):\n",
        "        # You might need to get the original model name from model_paths_dict\n",
        "        # This requires matching the model instance back to its name/path\n",
        "        # For simplicity, let's use a generic name for now.\n",
        "        model_name = f\"Model {i+1}\"\n",
        "        # A more robust way would involve storing model names with instances during loading.\n",
        "\n",
        "        print(f\"  Evaluating {model_name}...\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Perform prediction for the individual model slice by slice\n",
        "            model_predictions = []\n",
        "            for b in range(x_batch_eval.shape[0]): # Iterate through batch\n",
        "                 volume_predictions = []\n",
        "                 for d in range(x_batch_eval.shape[2]): # Iterate through depth dimension\n",
        "                     x_slice = x_batch_eval[b:b+1, :, d:d+1, :, :].to(model.device, dtype=torch.float)\n",
        "                     x_slice = x_slice.squeeze(2) # Remove depth dim to get [1, C, H, W]\n",
        "\n",
        "                     # --- Add padding to make height and width divisible by the required divisor ---\n",
        "                     required_divisor = 32 # Use the largest required divisor observed\n",
        "                     h, w = x_slice.shape[2:]\n",
        "                     new_h = (h + required_divisor - 1) // required_divisor * required_divisor\n",
        "                     new_w = (w + required_divisor - 1) // required_divisor * required_divisor\n",
        "                     target_padded_shape_slice = (new_h, new_w)\n",
        "\n",
        "                     x_slice_padded = pad_or_crop_to_shape_2d(x_slice, target_padded_shape_slice)\n",
        "\n",
        "                     out_slice = model(x_slice_padded) # Model predicts probabilities for the slice [1, 1, H, W]\n",
        "\n",
        "                     # Ensure output slice is 4D [1, C, H, W] and align spatial dimensions\n",
        "                     while out_slice.ndim < 4:\n",
        "                         out_slice = out_slice.unsqueeze(0)\n",
        "\n",
        "                     # Align the model's output to the original target spatial shape\n",
        "                     target_spatial_shape_slice_for_alignment = target_spatial_shape_3d_eval[1:] # (H, W)\n",
        "                     out_slice_aligned = pad_or_crop_to_shape_2d(out_slice, target_spatial_shape_slice_for_alignment)\n",
        "\n",
        "                     # Threshold the prediction to get a binary mask for the slice\n",
        "                     pred_slice_binary = (out_slice_aligned > 0.5).float()\n",
        "\n",
        "                     # Add depth dimension back\n",
        "                     pred_slice_binary = pred_slice_binary.unsqueeze(2) # [1, 1, 1, H, W]\n",
        "                     volume_predictions.append(pred_slice_binary)\n",
        "\n",
        "                 # Stack slices back to form a 3D volume prediction for the sample\n",
        "                 volume_predictions = torch.cat(volume_predictions, dim=2) # Concatenate along depth [1, 1, D, H, W]\n",
        "                 model_predictions.append(volume_predictions)\n",
        "\n",
        "            # Concatenate predictions for the batch\n",
        "            model_predictions_batch = torch.cat(model_predictions, dim=0) # [B, 1, D, H, W]\n",
        "\n",
        "        # Calculate evaluation metrics for the batch prediction\n",
        "        # Ensure target mask has the correct shape [B, 1, D, H, W]\n",
        "        y_batch_eval_aligned = safe_unsqueeze_mask(y_batch_eval)\n",
        "\n",
        "        # Ensure both tensors have the same spatial dimensions for metric calculation\n",
        "        # This step is similar to the loss calculation alignment.\n",
        "        min_spatial_shape_eval = [min(a, b) for a, b in zip(model_predictions_batch.shape[2:], y_batch_eval_aligned.shape[2:])]\n",
        "        model_predictions_batch_aligned = model_predictions_batch[:, :, :min_spatial_shape_eval[0], :min_spatial_shape_eval[1], :min_spatial_shape_eval[2]]\n",
        "        y_batch_eval_aligned_cropped = y_batch_eval_aligned[:, :, :min_spatial_shape_eval[0], :min_spatial_shape_eval[1], :min_spatial_shape_eval[2]]\n",
        "\n",
        "\n",
        "        dice_batch = dice_score(model_predictions_batch_aligned, y_batch_eval_aligned_cropped)\n",
        "        # You can add AVD and F1 here if you have those functions\n",
        "        # avd_batch = absolute_volume_difference(model_predictions_batch_aligned.cpu().numpy(), y_batch_eval_aligned_cropped.cpu().numpy())\n",
        "        # f1_batch = lesion_wise_f1_score(model_predictions_batch_aligned.cpu().numpy(), y_batch_eval_aligned_cropped.cpu().numpy())\n",
        "\n",
        "        # Store the metrics in a dictionary for this model\n",
        "        metrics_entry = {\n",
        "            \"Model\": model_name,\n",
        "            \"Dice Score (Batch)\": dice_batch.item() # Use .item() to get scalar from tensor\n",
        "            # Add other metrics here\n",
        "            # \"AVD (Batch)\": avd_batch,\n",
        "            # \"Lesion-wise F1 (Batch)\": f1_batch\n",
        "        }\n",
        "        model_metrics_list.append(metrics_entry)\n",
        "\n",
        "# %% Display Metrics Table\n",
        "import pandas as pd\n",
        "\n",
        "if model_metrics_list:\n",
        "    df_metrics = pd.DataFrame(model_metrics_list)\n",
        "    print(\"\\nIndividual Model Evaluation Metrics (on a batch):\")\n",
        "    display(df_metrics) # Use display for a nice table in Colab\n",
        "\n",
        "# %% Rest of your code (Ensemble prediction, visualization)\n",
        "# ... (Your code for ensemble prediction and plotting) ..."
      ],
      "metadata": {
        "id": "AOnx0coVCPYy"
      },
      "id": "AOnx0coVCPYy",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "cell_execution_strategy": "setup",
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "39a290b1ebc248c386415f568b941476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2374ec3b5dd240baa86aebb441039120",
              "IPY_MODEL_7c7f493c7bf246d0a55fb2afd14de36b",
              "IPY_MODEL_4cd212ab4ef348f19225032415a399f8"
            ],
            "layout": "IPY_MODEL_8d571893a63843e5ae190b30b95bce7f"
          }
        },
        "2374ec3b5dd240baa86aebb441039120": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d879717e64f438e807d67c4db795aeb",
            "placeholder": "​",
            "style": "IPY_MODEL_51abac2b5993406ab1ae1cc8380e197c",
            "value": "config.json: 100%"
          }
        },
        "7c7f493c7bf246d0a55fb2afd14de36b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2c49cc64f394beebbb604e728e3b3fd",
            "max": 156,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_660214638995483986b0c915e6c4315f",
            "value": 156
          }
        },
        "4cd212ab4ef348f19225032415a399f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb3c85fa51ed418189aa4834a8581810",
            "placeholder": "​",
            "style": "IPY_MODEL_c40b9be0d0f84419b790e61b5de52e96",
            "value": " 156/156 [00:00&lt;00:00, 18.6kB/s]"
          }
        },
        "8d571893a63843e5ae190b30b95bce7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d879717e64f438e807d67c4db795aeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51abac2b5993406ab1ae1cc8380e197c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2c49cc64f394beebbb604e728e3b3fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "660214638995483986b0c915e6c4315f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb3c85fa51ed418189aa4834a8581810": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c40b9be0d0f84419b790e61b5de52e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f3afdfc254f49e1807bbeab903d4315": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_34ded42d82cc4eb985dea3067576dec7",
              "IPY_MODEL_c8d73cb633d94bd3bd0e423c3695b648",
              "IPY_MODEL_e0b8fedb8a434cfeb1f228b5a3bdb88f"
            ],
            "layout": "IPY_MODEL_d9c021f95e794e26a94e535b81c47a14"
          }
        },
        "34ded42d82cc4eb985dea3067576dec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6da4decc39040979b0dc053342a4a6c",
            "placeholder": "​",
            "style": "IPY_MODEL_5c3c4e81a9594d989a75c2f671a0b075",
            "value": "model.safetensors: 100%"
          }
        },
        "c8d73cb633d94bd3bd0e423c3695b648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a79712743dfa4ccdbdc17e8d0c862bec",
            "max": 87275112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cca10ec3af39425d873a8bb534146211",
            "value": 87275112
          }
        },
        "e0b8fedb8a434cfeb1f228b5a3bdb88f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b5f84b5b86946bfac06068f076a5d97",
            "placeholder": "​",
            "style": "IPY_MODEL_7d4bf38aff304b1390342ec33307cb5f",
            "value": " 87.3M/87.3M [00:00&lt;00:00, 368MB/s]"
          }
        },
        "d9c021f95e794e26a94e535b81c47a14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6da4decc39040979b0dc053342a4a6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c3c4e81a9594d989a75c2f671a0b075": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a79712743dfa4ccdbdc17e8d0c862bec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cca10ec3af39425d873a8bb534146211": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b5f84b5b86946bfac06068f076a5d97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d4bf38aff304b1390342ec33307cb5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}